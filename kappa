#!/usr/bin/env python3

###############################################################################
#                 KEY AMINOACID PATTERN-BASED PROTEIN ANALYZER                #
#                     KAPPA 1.1 -- Sequence search script                     #
###############################################################################

# =============================================================================
# PLEASE CITE:
#   Joly V and Matton DP (2015). KAPPA, a simple algorithm for discovery and
#   clustering of proteins defined by a key amino acid pattern: a case study
#   of the cysteine-rich proteins. Bioinformatics, 31(11), 1716--1723.
#   DOI: 10.1093/bioinformatics/btv047
# =============================================================================
# LICENSE
#   This program is under the GNU General Public License version 3.0 (GPLv3)
#   More information at: http://www.gnu.org/copyleft/gpl.html
# =============================================================================
# CONTACT
#   Valentin Joly, doctorant / Ph.D. student
#   Université de Montréal - Institut de Recherche en Biologie Végétale
#   4101, rue Sherbrooke Est
#   Montréal (QC) H2J 2B2
#   valentin.joly (at) umontreal.ca
# =============================================================================

import argparse
import csv
import datetime
import itertools
import math
import multiprocessing
import os
import re
import shutil
import subprocess
import sys

sys.setrecursionlimit(20000)


def print_stdout(message, level, end, condition):
    if condition:
        time = datetime.datetime.now().strftime('%H:%M:%S')
        print(time, end='')
        spacer = ' ' * (level + 1) * 4
        print(spacer, end='')
        print(message)
        print(end, end='')


def print_stderr(message, level, end, condition):
    if condition:
        time = datetime.datetime.now().strftime('%H:%M:%S')
        spacer = ' ' * (level + 1) * 4
        sys.stderr.write(time + spacer + message + end + '\n')


def check_program_path(program):
    fpath, fname = os.path.split(program)
    if fpath:
        if os.path.isfile(fpath) and os.access(fpath, os.X_OK):
            return True
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            path = path.strip('"')
            exe_file = os.path.join(path, program)
            if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):
                return True
    return False


def isfasta(file_name):
    fasta = False
    if '.' in file_name:
        ext = file_name[file_name.rfind('.'):]
        ext = ext.lower()
        if ext in ['.fa', '.fas', '.fasta', '.fna', '.faa',
                   '.ffn', 'frn', 'seq']:
            fasta = True
    return fasta


def partition_dict(dictionary, nb_parts):
    N = len(dictionary)
    M = nb_parts
    keys = list(dictionary.keys())
    subdicts = []

    for i in range(N % M):
        subdict = {}
        begin = int(i * (1 + (N - N % M) // M))
        end = int((i + 1) * (1 + (N - N % M) // M))
        for j in range(begin, end):
            subdict[keys[j]] = dictionary[keys[j]]
        subdicts.append(subdict)

    for i in range(N % M, M):
        subdict = {}
        begin = N % M + int(i * ((N - N % M) // M))
        end = N % M + int((i + 1) * ((N - N % M) / M))
        for j in range(begin, end):
            subdict[keys[j]] = dictionary[keys[j]]
        subdicts.append(subdict)

    return subdicts


def partition_list(mainlist, nb_parts):
    N = len(mainlist)
    M = nb_parts

    sublists = []

    for i in range(N % M):
        sublist = []
        begin = int(i * (1 + (N - N % M) // M))
        end = int((i + 1) * (1 + (N - N % M) // M))
        for j in range(begin, end):
            sublist.append(mainlist[j])
        sublists.append(sublist)

    for i in range(N % M, M):
        sublist = []
        begin = N % M + int(i * ((N - N % M) // M))
        end = N % M + int((i + 1) * ((N - N % M) / M))
        for j in range(begin, end):
            sublist.append(mainlist[j])
        sublists.append(sublist)

    return sublists


def mean(values_list):
    values = []
    for value in values_list:
        if value != 'NA':
            values.append(value)
    if values != []:
        mean = 0
        for value in values:
            mean += value
        mean /= len(values)
        if mean != 0:
            mean = round(mean, 4 - int(math.floor(math.log10(mean))) - 1)
    else:
        mean = 'NA'
    return mean


def stdev(values_list):
    values = []
    for value in values_list:
        if value != 'NA':
            values.append(value)
    if values != []:
        m = mean(values)
        sd = 0
        for value in values:
            sd += (value - m)**2
        sd = (sd / len(values))**0.5
        if sd != 0:
            sd = round(sd, 4 - int(math.floor(math.log10(sd))) - 1)
    else:
        sd = 'NA'
    return sd


def mini(values_list):
    values = []
    for value in values_list:
        if value != 'NA':
            values.append(value)
    if values != []:
        mini = min(values)
    else:
        mini = 'NA'
    return mini


def maxi(values_list):
    values = []
    for value in values_list:
        if value != 'NA':
            values.append(value)
    if values != []:
        maxi = max(values)
    else:
        maxi = 'NA'
    return maxi


def read_config_file(config_file_name):
    errors = []
    config = {
        'signalp': None,
        'secretomep': None,
        'blastp': None,
        'makeblastdb': None,
        'c': None,
        's': None,
        't': None,
        'u': None,
        'U': None,
        'evalue': None,
        'word_size': None,
        'gapopen': None,
        'gapextend': None,
        'matrix': None,
        'comp_based_stats': None,
        'seg': None,
        'soft_masking': None,
        'lcase_masking': None,
        'culling_limit': None,
        'best_hit_overhang': None,
        'best_hit_score_edge': None,
        'max_target_seqs': None,
        'dbsize': None,
        'searchsp': None,
        'max_hsps': None,
        'sum_statistics': None,
        'xdrop_ungap': None,
        'xdrop_gap': None,
        'xdrop_gap_final': None,
        'window_size': None,
        'ungapped': None,
        'use_sw_tback': None
    }

    config_file = open(config_file_name, 'r')

    i = 0
    for line in config_file:
        i += 1
        if '#' in line:
            line = line[:line.index('#')]
        line = line.strip()
        if line != '':
            if '=' in line:
                option = line[:line.index('=')]
                value = line[line.index('=') + 1:]
                option = option.strip()
                value = value.strip()
            else:
                option = line
                value = ''

            try:
                if option in config.keys():
                    config[option] = value
                else:
                    raise ValueError
            except ValueError:
                errors.append(
                    'ERROR: KAPPA configuration file: Bad line/option name '
                    'formatting. Offending value: '
                    + option + ' (line ' + str(i) + ')')

    blastp_path, makeblastdb_path = '', ''
    blastp_options = []

    try:
        if config['blastp'] is None or config['blastp'] == '':
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: No path to BLASTp executable '
            'is provided.'
        )

    try:
        if (config['blastp'] is not None and config['blastp'] != '' and
                not check_program_path(config['blastp'])):
            raise ValueError
        else:
            blastp_path = config['blastp']
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: Path to BLASTp executable is '
            'not valid. Offending value: ' + config['blastp'])

    try:
        if config['makeblastdb'] is None or config['makeblastdb'] == '':
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: No path to makeblastdb '
            'executable is provided.')

    try:
        if (config['makeblastdb'] is not None and
                config['makeblastdb'] != '' and
                not check_program_path(config['makeblastdb'])):
            raise ValueError
        else:
            makeblastdb_path = config['makeblastdb']
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: Path to makeblastb executable '
            'is not valid. Offending value: ' + config['makeblastdb'])

    try:
        if config['evalue'] is not None:
            if float(config['evalue']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-evalue', config['evalue']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -evalue must be '
            'a real number greater than zero. Offending value: '
            + config['evalue'])

    try:
        if config['word_size'] is not None:
            if int(config['word_size']) < 1:
                raise ValueError
            else:
                blastp_options.extend(['-word_size', config['word_size']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -word_size must '
            'be an integer number greater than 1. Offending value: '
            + config['word_size'])

    try:
        if config['gapopen'] is not None:
            if int(config['gapopen']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-gapopen', config['gapopen']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -gapopen must '
            'be an integer number greater than 0. Offending value: '
            + config['gapopen'])

    try:
        if config['gapextend'] is not None:
            if int(config['gapextend']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-gapextend', config['gapextend']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -gapextend must '
            'be an integer number greater than 0. Offending value: '
            + config['gapextend'])

    try:
        if config['matrix'] is not None:
            if config['matrix'] not in [
                    'PAM30', 'PAM70', 'PAM250', 'BLOSUM45', 'BLOSUM50',
                    'BLOSUM62', 'BLOSUM80', 'BLOSUM90'
            ]:
                raise ValueError
            else:
                blastp_options.extend(['-matrix', config['matrix']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -matrix must '
            'be equal to PAM30, PAM70, PAM250, BLOSUM45, BLOSUM50, BLOSUM62, '
            'BLOSUM80, or BLOSUM90. Offending value: '
            + config['matrix'])

    try:
        if config['comp_based_stats'] is not None:
            if int(config['comp_based_stats']) < 0 or int(
                    config['comp_based_stats']) > 3:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-comp_based_stats', config['comp_based_stats']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -comp_based_stats '
            'must be equal to 0, 1, 2 or 3. Offending value: '
            + config['comp_based_stats'])

    try:
        if config['seg'] is not None:
            if config['seg'] not in ['yes', 'no', 'window locut hitcut']:
                raise ValueError
            else:
                blastp_options.extend(['-seg', config['seg']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -seg must be '
            'equal to "yes", "no" or "window locut hitcut". Offending value: '
            + config['seg'])

    try:
        if config['soft_masking'] is not None:
            if config['soft_masking'] not in ['true', 'false']:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-soft_masking', config['soft_masking']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -soft_masking '
            'must be equal to "true" or "false". Offending value: '
            + config['soft_masking'])

    try:
        if config['lcase_masking'] is not None:
            if config['lcase_masking'] != '':
                raise ValueError
            else:
                blastp_options.extend(
                    ['-lcase_masking', config['lcase_masking']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -lcase_masking '
            'must not be associated to any value. Offending value: '
            + config['lcase_masking'])

    try:
        if (config['culling_limit'] is not None and
                config['best_hit_overhang'] is not None):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp options -culling_limit '
            'and -best_hit_overhang are not compatible.')

    try:
        if (config['culling_limit'] is not None and
                config['best_hit_score_edge'] is not None):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp options -culling_limit '
            'and -best_hit_score_edge are not compatible.')

    try:
        if config['culling_limit'] is not None:
            if int(config['culling_limit']) < 0:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-culling_limit', config['culling_limit']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -culling_limit '
            'must be an integer number greater than 0. Offending value: '
            + config['culling_limit'])

    try:
        if config['best_hit_overhang'] is not None:
            if float(config['best_hit_overhang']) < 0 or float(
                    config['best_hit_overhang']) > 0.5:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-best_hit_overhang', config['best_hit_overhang']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option '
            '-best_hit_overhang must be a real number comprised '
            'between 0 and 0.5 inclusive. Offending value: '
            + config['best_hit_overhang'])

    try:
        if config['best_hit_score_edge'] is not None:
            if float(config['best_hit_score_edge']) < 0 or float(
                    config['best_hit_score_edge']) > 0.5:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-best_hit_score_edge', config['best_hit_score_edge']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option '
            '-best_hit_score_edge must be a real number comprised '
            'between 0 and 0.5 inclusive. Offending value: '
            + config['best_hit_score_edge'])

    try:
        if config['max_target_seqs'] is not None:
            if int(config['max_target_seqs']) <= 0:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-max_target_seqs', config['max_target_seqs']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -max_target_seqs '
            'must be an integer number greater than 0. Offending value: '
            + config['max_target_seqs'])

    try:
        if config['dbsize'] is not None:
            if int(config['dbsize']) <= 0:
                raise ValueError
            else:
                blastp_options.extend(['-dbsize', config['dbsize']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -dbsize must be '
            'an integer number greater than 0. Offending value: '
            + config['dbsize'])

    try:
        if config['searchsp'] is not None:
            if int(config['searchsp']) <= 0:
                raise ValueError
            else:
                blastp_options.extend(['-searchsp', config['searchsp']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -searchsp must '
            'be an integer number greater than 0. Offending value: '
            + config['searchsp'])

    try:
        if config['max_hsps'] is not None:
            if int(config['max_hsps']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-max_hsps', config['max_hsps']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -max_hsps must '
            'be an integer number greater than 0. Offending value: '
            + config['max_hsps'])

    try:
        if config['sum_statistics'] is not None:
            if config['sum_statistics'] != '':
                raise ValueError
            else:
                blastp_options.extend(
                    ['-sum_statistics', config['sum_statistics']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -sum_statistics '
            'must not be associated to any value. Offending value: '
            + config['sum_statistics'])

    try:
        if config['xdrop_ungap'] is not None:
            if int(config['xdrop_ungap']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-xdrop_ungap', config['xdrop_ungap']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -xdrop_ungap must '
            'be an integer number greater than zero. Offending value: '
            + config['xdrop_ungap'])

    try:
        if config['xdrop_gap'] is not None:
            if int(config['xdrop_gap']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-xdrop_gap', config['xdrop_gap']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -xdrop_gap must '
            'be an integer number greater than zero. Offending value: '
            + config['xdrop_gap'])

    try:
        if config['xdrop_gap_final'] is not None:
            if int(config['xdrop_gap_final']) < 0:
                raise ValueError
            else:
                blastp_options.extend(
                    ['-xdrop_gap_final', config['xdrop_gap_final']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -xdrop_gap_final '
            'must be an integer number greater than zero. Offending value: '
            + config['xdrop_gap_final'])

    try:
        if config['window_size'] is not None:
            if int(config['window_size']) < 0:
                raise ValueError
            else:
                blastp_options.extend(['-window_size', config['window_size']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -window_size must '
            'be an integer number greater than zero. Offending value: '
            + config['window_size'])

    try:
        if config['ungapped'] is not None:
            if config['ungapped'] != '':
                raise ValueError
            else:
                blastp_options.extend(['-ungapped', config['ungapped']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -ungapped must '
            'not be associated to any value. Offending value: '
            + config['ungapped'])

    try:
        if config['use_sw_tback'] is not None:
            if config['use_sw_tback'] != '':
                raise ValueError
            else:
                blastp_options.extend(
                    ['-use_sw_tback', config['use_sw_tback']])
    except ValueError:
        errors.append(
            'ERROR: KAPPA configuration file: BLASTp option -use_sw_tback '
            'must not be associated to any value. Offending value: '
            + config['use_sw_tback'])

    return blastp_path, makeblastdb_path, blastp_options, errors


def import_target_proteins(target_proteins_files, report, optimization):

    print_stdout(
        'Importing target proteins...',
        level=0,
        end='',
        condition=not optimization)

    initial_target_proteins = {}
    seqIDs = {}
    samples = {}
    fasta_headers = {}

    prot = 0

    for target_proteins_file in target_proteins_files:

        target_proteins_fasta = open(target_proteins_file, 'r')

        i = -1

        print_stdout(
            'Importing from ' + target_proteins_file + '...',
            level=1,
            end='',
            condition=not optimization)

        fasta_structure = re.compile('^>([^ ]*)[^\[\]]*\[([^\]]*)\].*$')

        for line in target_proteins_fasta:
            line = line.strip()

            if line != '':

                if line[0] == '>':
                    fasta_ID = fasta_structure.match(line)

                    if fasta_ID is not None:
                        i += 1
                        prot += 1
                        simple_ID = 'prot' + str(prot)
                        seqIDs[simple_ID] = fasta_ID.group(1)
                        samples[simple_ID] = fasta_ID.group(2)
                        fasta_headers[simple_ID] = line[1:]
                        initial_target_proteins[simple_ID] = ''

                    else:
                        sys.stderr.write('ERROR: File ' +
                                         target_proteins_file +
                                         ' does not respect FASTA format.\n')
                        sys.stderr.write(
                            '       Sequence identification lines should look '
                            'like:\n')
                        sys.stderr.write(
                            '       >sequence_identification [species/sample] '
                            'description\n\n')
                        sys.exit(1)

                else:
                    if i > -1:
                        initial_target_proteins[simple_ID] += line.upper()

        target_proteins_fasta.close()

        print_stdout('Done.', level=2, end='', condition=not optimization)

    if len(initial_target_proteins) == 0:
        sys.stderr.write('ERROR: No input sequence was found in ' +
                         target_proteins_file + '.\n')
        sys.exit(1)

    report['imported_proteins_clustering'] = len(initial_target_proteins)

    print_stdout(
        'Done. ' + str(len(initial_target_proteins)) + ' proteins imported',
        level=1,
        end='\n',
        condition=not optimization)

    return initial_target_proteins, seqIDs, samples, fasta_headers, report


def filter_target_proteins(initial_target_proteins, fasta_headers, minlength,
                           maxlength, key_residue, minkey, maxkey, report,
                           optimization):

    print_stdout(
        'Filtering target proteins...',
        level=0,
        end='',
        condition=not optimization)

    target_proteins = {}

    for seqID in initial_target_proteins.keys():

        target_protein = initial_target_proteins[seqID]

        test = True

        if len(target_protein) < minlength:
            test = False
        if maxlength is not None and len(target_protein) > maxlength:
            test = False

        n = target_protein.count(key_residue)
        if n < minkey:
            test = False
        if maxkey is not None and n > maxkey:
            test = False

        if test:
            target_proteins[seqID] = target_protein

    initial_target_proteins = {}

    report['retained_proteins'] = len(target_proteins)
    report['rejected_proteins'] = (report['imported_proteins_clustering'] -
                                   report['retained_proteins'])

    if len(target_proteins) > 0:
        print_stdout(
            'Done. ' + str(len(target_proteins)) + ' proteins kept',
            level=1,
            end='\n',
            condition=not optimization)

    else:
        print_stderr(
            'ERROR: No sequence passed filtering step. Retry with less '
            'stringent parameters.',
            level=1,
            end='\n',
            condition=not optimization)
        print_stdout(
            'WARNING: No sequence passed filtering step.',
            level=1,
            end='',
            condition=optimization)

        if not report['b']:
            report['query_patterns'] = 'NA'

            if report['q'] is None:
                report['clusters'] = 'NA'
                report['clustering_singletons'] = 'NA'
                report['proteins_per_cluster'] = ('NA', 'NA', 'NA', 'NA')
                report['clusters_per_protein'] = ('NA', 'NA', 'NA', 'NA')

        if report['s']:
            report['imported_clusters'] = 'NA'
            report['imported_proteins_subclustering'] = 'NA'
            report['nb_subclusters'] = 'NA'
            report['nb_singletons_subclusters'] = 'NA'
            report['proteins_per_subcluster'] = ('NA', 'NA', 'NA', 'NA')
            report['subclusters_per_protein'] = ('NA', 'NA', 'NA', 'NA')
            report['edges_per_subcluster'] = ('NA', 'NA', 'NA', 'NA')
            report['samples_per_subcluster'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_densities'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_densities_intra'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_densities_inter'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_hit_pidents'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_hit_pidents_intra'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_hit_pidents_inter'] = ('NA', 'NA', 'NA', 'NA')
            report['subcluster_prot_lengths'] = ('NA', 'NA', 'NA', 'NA')

    return target_proteins, report


def extract_target_patterns(target_proteins, key_residue, nterm, cterm,
                            optimization):

    print_stdout(
        'Extracting target patterns from target proteins...',
        level=0,
        end='',
        condition=not optimization)

    target_patterns = {}

    for seqID in target_proteins.keys():

        target_protein = target_proteins[seqID]

        target_pattern = [0]

        for aa in target_protein:
            if aa == key_residue:
                target_pattern.append(0)
            elif aa != '-' and aa != '*':
                target_pattern[-1] += 1

        for i in range(nterm):
            target_pattern[i] = '.'

        for i in range(len(target_pattern) - cterm, len(target_pattern)):
            target_pattern[i] = '.'

        target_pattern_ID = seqID
        target_patterns[target_pattern_ID] = tuple(target_pattern)

    print_stdout(
        'Done. ' + str(len(target_patterns)) + ' target patterns added',
        level=1,
        end='\n',
        condition=not optimization)

    return target_patterns


def extract_de_novo_query_patterns(target_patterns, nterm, cterm, seqIDs,
                                   report, optimization):

    print_stdout(
        'Extracting query patterns from target proteins...',
        level=0,
        end='',
        condition=not optimization)

    query_patterns = {}
    query_IDs = {}

    n = 0

    for target_pattern_ID in target_patterns.keys():
        target_pattern = target_patterns[target_pattern_ID]

        query_pattern = []
        for item in target_pattern:
            if type(item) == int:
                query_pattern.append((item, item))
            else:
                query_pattern.append(item)
        query_pattern = tuple(query_pattern)

        query_pattern_exists = False
        for query_pattern_ID in query_patterns.keys():
            if query_pattern == query_patterns[query_pattern_ID]:
                query_IDs[query_pattern_ID].append(target_pattern_ID)
                query_pattern_exists = True
                break
        if not query_pattern_exists:
            n += 1
            query_pattern_ID = 'pattern_' + str(n)
            query_patterns[query_pattern_ID] = query_pattern
            query_IDs[query_pattern_ID] = [target_pattern_ID]

        if len(query_patterns) % 10000 == 0:
            print_stdout(
                str(len(query_patterns)) + ' query patterns extracted',
                level=1)

    report['query_patterns'] = len(query_patterns)

    print_stdout(
        'Done. ' + str(n) + ' query patterns extracted',
        level=1,
        end='\n',
        condition=not optimization)

    return query_patterns, query_IDs, report


def extract_query_pattern(query_pattern_string, nterm, cterm, key_residue):

    query_pattern = [0]
    i = 0
    while i < len(query_pattern_string):
        if query_pattern_string[i] == key_residue:
            query_pattern.append(0)
            i += 1
        else:
            if i + 1 == len(query_pattern_string
                            ) or query_pattern_string[i + 1] != '{':
                if type(query_pattern[-1]) == int:
                    query_pattern[-1] += 1
                elif type(query_pattern[-1]) == list:
                    query_pattern[-1][0] += 1
                    query_pattern[-1][1] += 1
                i += 1
            else:
                i += 2
                number = query_pattern_string[i]
                while query_pattern_string[i + 1] != '}':
                    i += 1
                    number += query_pattern_string[i]
                i += 2
                if number == 'n':
                    query_pattern[-1] = '*'
                elif '-' not in number:
                    if type(query_pattern[-1]) == int:
                        query_pattern[-1] += int(number)
                    elif type(query_pattern[-1]) == list:
                        query_pattern[-1][0] += int(number)
                        query_pattern[-1][1] += int(number)
                else:
                    low = int(number[:number.index('-')])
                    high = int(number[(number.index('-') + 1):])
                    if type(query_pattern[-1]) == int:
                        init = query_pattern[-1]
                        query_pattern[-1] = [low + init, high + init]
                    elif type(query_pattern[-1]) == list:
                        init_low = query_pattern[-1][0]
                        init_high = query_pattern[-1][1]
                        query_pattern[-1] = [low + init_low, high + init_high]

    for i in range(nterm):
        query_pattern[i] = '.'

    for i in range(len(query_pattern) - cterm, len(query_pattern)):
        query_pattern[i] = '.'

    for i in range(len(query_pattern)):
        if type(query_pattern[i]) == int:
            query_pattern[i] = (query_pattern[i], query_pattern[i])
        elif type(query_pattern[i]) == list:
            query_pattern[i] = tuple(query_pattern[i])

    query_pattern = tuple(query_pattern)

    return query_pattern


def check_query_pattern(pattern):

    test = not bool(re.compile(r'[^nA-Z0-9\-\{\}]').search(pattern))

    if test:
        opened = False
        for i in range(len(pattern)):
            ch = pattern[i]
            if ch == '{':
                if opened:
                    test = False
                    break
                else:
                    opened = True
                    value = ''
            elif ch == '}':
                if not opened:
                    test = False
                    break
                else:
                    opened = False
                    if value != 'n' and bool(
                            re.compile(r'[^0-9\-]').search(value)):
                        test = False
                        break
                    if test and '-' in value:
                        values = value.split('-')
                        if len(values) != 2:
                            test = False
                            break
                        elif not values[0].isdigit():
                            test = False
                            break
                        elif not values[1].isdigit():
                            test = False
                            break
                        elif int(values[0]) > int(values[1]):
                            test = False
                            break
            elif opened:
                value += ch
            elif bool(re.compile(r'[^A-Z]').search(ch)):
                test = False
                break

            if i == len(pattern) - 1 and ch != '}' and opened:
                test = False

    return test


def extract_external_query_patterns(query_patterns_file_names, nterm, cterm,
                                    key_residue, report, optimization):

    print_stdout(
        'Importing reference query patterns...',
        level=0,
        end='',
        condition=not optimization)

    query_patterns = {}
    query_IDs = {}

    line_structure = re.compile('^([^ \t]+)[ \t]+([^ \t]+)$')

    for query_patterns_file_name in query_patterns_file_names:

        print_stdout(
            'Importing from ' + query_patterns_file_name + '...',
            level=1,
            end='',
            condition=not optimization)

        query_patterns_file = open(query_patterns_file_name, 'r')

        i = 0
        for line in query_patterns_file:
            i += 1
            if '#' in line:
                line = line[:line.index('#')]
            line = line.strip()
            if line != '':
                line_match = line_structure.match(line)
                try:
                    if line_match is not None:
                        query_pattern_ID = line_match.group(1)
                        query_pattern_string = line_match.group(2)
                        if (query_pattern_ID is None or
                                query_pattern_string is None):
                            raise ValueError
                            break
                        elif not check_query_pattern(query_pattern_string):
                            raise ValueError
                            break
                        else:
                            query_patterns[
                                query_pattern_ID] = extract_query_pattern(
                                    query_pattern_string, nterm, cterm,
                                    key_residue)
                            query_IDs[query_pattern_ID] = [query_pattern_ID]
                    else:
                        raise ValueError
                        break
                except ValueError:
                    print_stderr(
                        'ERROR: Query patterns file: Bad formatting in line ' +
                        str(i) + '.\n',
                        level=1,
                        end='\n',
                        condition=not optimization or optimization)
                    sys.exit(1)

        query_patterns_file.close()

        print_stdout('Done.', level=2, end='', condition=not optimization)

    report['query_patterns'] = len(query_patterns)

    print_stdout(
        'Done. ' + str(len(query_patterns)) + ' query patterns extracted',
        level=1,
        end='\n',
        condition=not optimization)

    return query_patterns, query_IDs, report


def key_loss(pattern, loss):

    if len(pattern) - 2 < loss:
        nlost = len(pattern) - 2
    else:
        nlost = loss

    alt_patterns = []
    for n in range(0, nlost + 1):
        key_combs = [
            key_comb for key_comb in itertools.combinations(
                [key_index for key_index in range(len(pattern) - 1)], n)
        ]
        for key_comb in key_combs:
            alt_pattern = list(pattern)
            for k in range(len(key_comb)):
                key_index = key_comb[k] - k

                left_block = alt_pattern[key_index]
                right_block = alt_pattern[key_index + 1]
                if left_block == '.' or right_block == '.':
                    fused_block = '.'
                elif left_block == '*' or right_block == '*':
                    fused_block = '*'
                elif type(left_block) == int and type(left_block) == int:
                    fused_block = left_block + right_block + 1
                elif type(left_block) == tuple and type(left_block) == tuple:
                    fused_block = (left_block[0] + right_block[0] + 1,
                                   left_block[1] + right_block[1] + 1)
                alt_pattern[key_index] = fused_block
                alt_pattern.pop(key_index + 1)
            alt_patterns.append(tuple(alt_pattern))

    alt_patterns.pop(0)

    return alt_patterns


def key_gain_part(pattern):

    alt_patterns = []

    for i in range(len(pattern)):
        if pattern[i] == '*':
            alt_pattern = pattern[:i]
            alt_pattern.extend([pattern[i], pattern[i]])
            alt_pattern.extend(pattern[i + 1:])
            alt_patterns.append(alt_pattern)

        elif type(pattern[i]) == int and pattern[i] > 0:
            for j in range(pattern[i]):
                alt_pattern = pattern[:i]
                alt_pattern.extend([j, pattern[i] - 1 - j])
                alt_pattern.extend(pattern[i + 1:])
                alt_patterns.append(alt_pattern)

        elif type(pattern[i]) == tuple and pattern[i] != (0, 0):
            for j in range(0, pattern[i][1]):
                alt_pattern = pattern[:i]
                alt_pattern.extend([(j, j), (max(0, pattern[i][0] - 1 - j),
                                             pattern[i][1] - 1 - j)])
                alt_pattern.extend(pattern[i + 1:])
                alt_patterns.append(alt_pattern)

    return alt_patterns


def key_gain(pattern, gain):

    alt_patterns = [list(pattern)]

    for g in range(1, gain + 1):
        new_alt_patterns = []
        for alt_pattern in alt_patterns:
            new_alt_patterns.extend(key_gain_part(alt_pattern))
        alt_patterns.extend(new_alt_patterns)

    alt_patterns = [tuple(pattern) for pattern in alt_patterns]
    alt_patterns = list(set(alt_patterns))
    alt_patterns.sort(key=lambda s: len(s))
    alt_patterns.pop(0)

    return alt_patterns


def adjust_value(value, stringency):
    f_x = 1 / (1 + math.exp(20 * (stringency - value)))
    f_0 = 1 / (1 + math.exp(20 * stringency))
    f_1 = 1 / (1 + math.exp(20 * (stringency - 1)))
    return (f_x - f_0) / (f_1 - f_0)


def assess_alignment(query_pattern, target_pattern, identity_str, coverage_str,
                     allow_inclusion, de_novo, export_scores, score_thr):

    if len(query_pattern) >= len(target_pattern):
        P, p = query_pattern, target_pattern
    else:
        P, p = target_pattern, query_pattern

    L, l = len(P), len(p)

    if allow_inclusion:
        if de_novo:
            L_ref = l
        else:
            L_ref = len(query_pattern)
    else:
        L_ref = L

    best_local_score = -1

    shifts = [shift for shift in range(L)]
    for shift in range(1, l):
        shifts.insert(shifts.index(shift) + 1, -1 * shift)

    for shift in shifts:
        pg, pd, Pg, Pd = [], [], [], []

        if shift < 0:
            Pg = ['-' for i in range(abs(shift))]
        else:
            pg = ['-' for i in range(shift)]

        if shift < L - l + 1:
            pd = ['-' for i in range(L - l - shift)]
        else:
            Pd = ['-' for i in range(shift - L + l)]

        a = pg
        a.extend(p)
        a.extend(pd)

        A = Pg
        A.extend(P)
        A.extend(Pd)

        aligned_blocks = 0
        aligned_values = 0
        identity_adj = 0.0

        for i in range(len(A)):
            if A[i] != '-' and a[i] != '-':
                aligned_blocks += 1
                if A[i] != '.' and a[i] != '.':
                    aligned_values += 1

                    if A[i] == '*' or a[i] == '*':
                        local_identity = 1

                    elif type(A[i]) == tuple and type(a[i]) == int:
                        if A[i][0] <= a[i] <= A[i][1]:
                            local_identity = 1
                        elif a[i] < A[i][0]:
                            local_identity = 1 - (A[i][0] - a[i]) / (
                                A[i][0] + a[i])
                        elif a[i] > A[i][1]:
                            local_identity = 1 - (a[i] - A[i][1]) / (
                                A[i][1] + a[i])

                    elif type(A[i]) == int and type(a[i]) == tuple:
                        if a[i][0] <= A[i] <= a[i][1]:
                            local_identity = 1
                        elif A[i] < a[i][0]:
                            local_identity = 1 - (a[i][0] - A[i]) / (
                                a[i][0] + A[i])
                        elif A[i] > a[i][1]:
                            local_identity = 1 - (A[i] - a[i][1]) / (
                                a[i][1] + A[i])

                    local_identity_adj = adjust_value(local_identity,
                                                      identity_str)
                    identity_adj += local_identity_adj

        if aligned_values > 0:
            identity_adj /= aligned_values

        coverage = aligned_blocks / L_ref
        coverage_adj = adjust_value(coverage, coverage_str)

        score = coverage_adj * identity_adj
        if score > best_local_score:
            best_local_score = score
            best_local_coverage = coverage_adj
            best_local_identity = identity_adj

    return best_local_score, best_local_coverage, best_local_identity


def map_patterns_part(p, query_patterns, target_patterns, identity_str,
                      coverage_str, persistence_str, score_thr, gain, loss,
                      allow_inclusion, de_novo, minkey, export_scores, queue):

    mapping_scores = {}

    x = 0
    percs = []

    for query_pattern_ID in query_patterns.keys():
        query_pattern = query_patterns[query_pattern_ID]

        for query_ID in query_IDs[query_pattern_ID]:
            mapping_scores[query_ID] = {}

        for target_pattern_ID in target_patterns.keys():

            x += 1
            perc = (100 * x) // (len(query_patterns) * len(target_patterns))

            if perc not in percs and perc % 5 == 0 and perc > 0:
                print_stdout(
                    'Processor ' + str(p) + ' has parsed ' + str(perc) +
                    '% values.',
                    level=1,
                    end='',
                    condition=not optimization)
                percs.append(perc)

            target_pattern = target_patterns[target_pattern_ID]

            best_score, best_coverage, best_identity = assess_alignment(
                query_pattern, target_pattern, identity_str, coverage_str,
                allow_inclusion, de_novo, export_scores, score_thr)
            best_persistence = 1.0

            if (gain > 0 or loss > 0) and (export_scores or
                                           (not export_scores
                                            and best_score < score_thr)):

                alt_query_patterns = []
                if gain > 0:
                    alt_query_patterns.extend(key_gain(query_pattern, gain))
                if loss > 0:
                    alt_query_patterns.extend(key_loss(query_pattern, loss))

                for alt_query_pattern in alt_query_patterns:
                    alt_score, alt_coverage, alt_identity = assess_alignment(
                        alt_query_pattern, target_pattern, identity_str,
                        coverage_str, allow_inclusion, de_novo, export_scores,
                        score_thr)
                    persistence = 1 - 1.0 * (
                        abs(len(query_pattern) -
                            len(alt_query_pattern))) / len(query_pattern)
                    persistence_adj = adjust_value(persistence,
                                                   persistence_str)
                    alt_score *= persistence_adj
                    if alt_score > best_score:
                        best_score = alt_score
                        best_coverage = alt_coverage
                        best_identity = alt_identity
                        best_persistence = persistence_adj

            for query_ID in query_IDs[query_pattern_ID]:
                mapping_scores[query_ID][target_pattern_ID] = [
                    best_score, best_coverage, best_identity, best_persistence
                ]

    queue.put(mapping_scores)


def map_patterns(query_patterns, target_patterns, identity_str, coverage_str,
                 persistence_str, score_thr, gain, loss, allow_inclusion,
                 de_novo, minkey, export_scores, threads, optimization):

    print_stdout(
        'Mapping query to target patterns...',
        level=0,
        end='',
        condition=not optimization)

    target_patterns_parts = partition_dict(target_patterns, threads)

    mapping_scores = {}

    queue = multiprocessing.Queue()
    processes = []
    p = 0
    for target_patterns_part in target_patterns_parts:
        p += 1
        process = multiprocessing.Process(
            target=map_patterns_part,
            args=(p, query_patterns, target_patterns_part, identity_str,
                  coverage_str, persistence_str, score_thr, gain, loss,
                  allow_inclusion, de_novo, minkey, export_scores, queue))
        process.start()
        processes.append(process)

    for p in range(len(processes)):
        mapping_scores_part = queue.get()
        for query_ID in mapping_scores_part.keys():
            if query_ID not in mapping_scores.keys():
                mapping_scores[query_ID] = {}
            for target_ID in mapping_scores_part[query_ID].keys():
                mapping_scores[query_ID][target_ID] = mapping_scores_part[
                    query_ID][target_ID]

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return mapping_scores


def export_mapping_scores(mapping_scores, seqIDs, de_novo, optimization):

    print_stdout(
        'Exporting mapping scores...',
        level=0,
        end='',
        condition=not optimization)

    mapping_scores_file_name = 'Mapping_results_' + file_id + '.txt'
    mapping_scores_file = open(mapping_scores_file_name, 'w')
    mapping_scores_table = csv.writer(mapping_scores_file, dialect='excel-tab')
    mapping_scores_table.writerow(
        ['Query sequence ID', 'Target sequence ID', 'Score', 'C', 'I', 'P'])
    mapping_scores_table.writerow([])

    exported_scores = {}
    for query_ID in mapping_scores.keys():
        if de_novo:
            query_key = seqIDs[query_ID]
        else:
            query_key = query_ID
        exported_scores[query_key] = {}
        for target_ID in mapping_scores[query_ID]:
            score = str(round(mapping_scores[query_ID][target_ID][0],
                              4)).ljust(6, '0')
            coverage = str(round(mapping_scores[query_ID][target_ID][1],
                                 4)).ljust(6, '0')
            identity = str(round(mapping_scores[query_ID][target_ID][2],
                                 4)).ljust(6, '0')
            persistence = str(
                round(mapping_scores[query_ID][target_ID][3], 4)).ljust(
                    6, '0')
            exported_scores[query_key][seqIDs[target_ID]] = [
                score, coverage, identity, persistence
            ]

    for query_ID in sorted(exported_scores.keys()):

        scores = {}
        for target_ID in exported_scores[query_ID].keys():
            score = exported_scores[query_ID][target_ID][0]
            if score in scores.keys():
                scores[score].append(target_ID)
            else:
                scores[score] = [target_ID]

        sorted_scores = list(scores.keys())
        sorted_scores.sort()
        sorted_scores = sorted_scores[::-1]

        sorted_target_IDs = []
        for score in sorted_scores:
            score_target_IDs = scores[score]
            score_target_IDs.sort()
            sorted_target_IDs.extend(score_target_IDs)

        for target_ID in sorted_target_IDs:
            mapping_result = [query_ID, target_ID]
            mapping_result.extend(exported_scores[query_ID][target_ID])
            mapping_scores_table.writerow(mapping_result)

        mapping_scores_table.writerow([])

    mapping_scores_file.close()

    print_stdout('Done.', level=1, end='\n', condition=not optimization)


def connect_sequences(mapping_scores_part, score_thr_pass1, queue):
    sequence_edges = {}
    for source_seqID in mapping_scores_part.keys():
        for target_seqID in mapping_scores_part[source_seqID].keys():
            score = mapping_scores_part[source_seqID][target_seqID][0]
            if score >= score_thr_pass1:
                edge_data = ['Pass1_edge']
                edge_data.extend(
                    mapping_scores_part[source_seqID][target_seqID])
                if source_seqID not in sequence_edges.keys():
                    sequence_edges[source_seqID] = {target_seqID: edge_data}
                else:
                    sequence_edges[source_seqID][target_seqID] = edge_data
                if target_seqID not in sequence_edges.keys():
                    sequence_edges[target_seqID] = {source_seqID: edge_data}
                else:
                    sequence_edges[target_seqID][source_seqID] = edge_data
    queue.put(sequence_edges)


def connect_pass1_clusters(pass1_clusters_part, pass1_clusters, mapping_scores,
                           score_thr_pass2, fusion_thr, queue):

    cluster_edges = {}
    sequence_edges = {}

    for source_clusterID in pass1_clusters_part.keys():
        for target_clusterID in pass1_clusters.keys():

            if source_clusterID == target_clusterID:
                if source_clusterID not in cluster_edges.keys():
                    cluster_edges[source_clusterID] = []
                if source_clusterID not in cluster_edges[source_clusterID]:
                    cluster_edges[source_clusterID].append(source_clusterID)
                seqIDs = pass1_clusters_part[source_clusterID]
                for source_seqID in seqIDs:
                    for target_seqID in seqIDs:
                        if source_seqID != target_seqID and score_thr_pass2 <= mapping_scores[source_seqID][target_seqID][0] < score_thr_pass1:
                            edge_data = ['Pass2_edge']
                            edge_data.extend(
                                mapping_scores[source_seqID][target_seqID])

                            if source_seqID not in sequence_edges.keys():
                                sequence_edges[source_seqID] = {}
                            sequence_edges[source_seqID][
                                target_seqID] = edge_data

                            if target_seqID not in sequence_edges.keys():
                                sequence_edges[target_seqID] = {}
                            sequence_edges[target_seqID][
                                source_seqID] = edge_data

            else:
                source_seqIDs = pass1_clusters_part[source_clusterID]
                target_seqIDs = pass1_clusters[target_clusterID]
                edges = {}
                for source_seqID in source_seqIDs:
                    for target_seqID in target_seqIDs:
                        if (mapping_scores[source_seqID][target_seqID][0] >=
                                score_thr_pass2):
                            edge_data = ['Pass2_edge']
                            edge_data.extend(
                                mapping_scores[source_seqID][target_seqID])

                            if source_seqID not in edges.keys():
                                edges[source_seqID] = {}
                            edges[source_seqID][target_seqID] = edge_data

                            if target_seqID not in edges.keys():
                                edges[target_seqID] = {}
                            edges[target_seqID][source_seqID] = edge_data

                if 1.0 * len(edges) / (
                        len(source_seqIDs) * len(target_seqIDs)) >= fusion_thr:
                    if source_clusterID not in cluster_edges.keys():
                        cluster_edges[source_clusterID] = []
                    if target_clusterID not in cluster_edges[source_clusterID]:
                        cluster_edges[source_clusterID].append(
                            target_clusterID)
                    if target_clusterID not in cluster_edges.keys():
                        cluster_edges[target_clusterID] = []
                    if source_clusterID not in cluster_edges[target_clusterID]:
                        cluster_edges[target_clusterID].append(
                            source_clusterID)

                    for source_seqID in edges.keys():
                        if source_seqID not in sequence_edges.keys():
                            sequence_edges[source_seqID] = {}
                        for target_seqID in edges[source_seqID].keys():
                            sequence_edges[source_seqID][target_seqID] = edges[
                                source_seqID][target_seqID]

    queue.put((cluster_edges, sequence_edges))


def recursive_search_clusters(node, edges, global_network, local_network):
    global_network.append(node)
    local_network.append(node)
    neighbor_nodes = edges[node]
    for neighbor_node in neighbor_nodes:
        if neighbor_node not in local_network:
            global_network, local_network = recursive_search_clusters(
                neighbor_node, edges, global_network, local_network)
    return global_network, local_network


def make_clusters(mapping_scores, threads, score_thr_pass1, score_thr_pass2,
                  fusion_thr, report, optimization):

    print_stdout(
        'Clustering sequences...', level=0, end='', condition=not optimization)

    node_attributes = {seqID: 'Singleton' for seqID in mapping_scores.keys()}

    mapping_scores_parts = partition_dict(mapping_scores, threads)
    queue = multiprocessing.Queue()
    processes = []
    for mapping_scores_part in mapping_scores_parts:
        process = multiprocessing.Process(
            target=connect_sequences,
            args=(mapping_scores_part, score_thr_pass1, queue))
        process.start()
        processes.append(process)
    sequence_edges_parts = []
    for p in range(len(processes)):
        sequence_edges_parts.append(queue.get())

    sequence_edges = {}
    for sequence_edges_part in sequence_edges_parts:
        for source_seqID in sequence_edges_part.keys():
            if source_seqID not in sequence_edges.keys():
                sequence_edges[source_seqID] = {}
            for target_seqID in sequence_edges_part[source_seqID].keys():
                sequence_edges[source_seqID][
                    target_seqID] = sequence_edges_part[source_seqID][
                        target_seqID]
    sequence_edges_parts = []

    pass1_clusters = {}
    nodes = list(sequence_edges.keys())
    x = 0
    global_network = []
    for node in nodes:
        if node not in global_network:
            local_network = []
            global_network, local_network = recursive_search_clusters(
                node, sequence_edges, global_network, local_network)
            x += 1
            pass1_clusters[x] = local_network

    for clusterID in pass1_clusters:
        cluster = pass1_clusters[clusterID]
        if len(cluster) > 1:
            for node in cluster:
                node_attributes[node] = 'Clustered_pass1'

    pass1_clusters_parts = partition_dict(pass1_clusters, threads)
    queue = multiprocessing.Queue()
    processes = []
    for pass1_clusters_part in pass1_clusters_parts:
        process = multiprocessing.Process(
            target=connect_pass1_clusters,
            args=(pass1_clusters_part, pass1_clusters, mapping_scores,
                  score_thr_pass2, fusion_thr, queue))
        process.start()
        processes.append(process)

    pass1_cluster_results_parts = []
    for p in range(len(processes)):
        pass1_cluster_results_parts.append(queue.get())

    pass1_cluster_edges = {}
    for pass1_cluster_results_part in pass1_cluster_results_parts:
        pass1_cluster_edges_part = pass1_cluster_results_part[0]
        for pass1_clusterID in pass1_cluster_edges_part.keys():
            if pass1_clusterID not in pass1_cluster_edges.keys():
                pass1_cluster_edges[
                    pass1_clusterID] = pass1_cluster_edges_part[
                        pass1_clusterID]
            else:
                pass1_cluster_edges[pass1_clusterID].extend(
                    pass1_cluster_edges_part[pass1_clusterID])

        sequence_edges_part = pass1_cluster_results_part[1]
        for source_seqID in sequence_edges_part.keys():
            if source_seqID not in sequence_edges.keys():
                sequence_edges[source_seqID] = {}
            for target_seqID in sequence_edges_part[source_seqID].keys():
                sequence_edges[source_seqID][
                    target_seqID] = sequence_edges_part[source_seqID][
                        target_seqID]

    pass1_cluster_results_parts = []

    for pass1_clusterID in pass1_cluster_edges.keys():
        pass1_cluster_edges[pass1_clusterID] = list(
            set(pass1_cluster_edges[pass1_clusterID]))

    pass2_clusters = {}
    nodes = list(pass1_cluster_edges.keys())
    x = 0
    if pass1_cluster_edges != {}:
        global_network = []
        for node in nodes:
            if node not in global_network:
                local_network = []
                global_network, local_network = recursive_search_clusters(
                    node, pass1_cluster_edges, global_network, local_network)
                x += 1
                pass2_clusters[x] = local_network
    else:
        pass2_clusters = pass1_clusters

    tmp_clusters = {}
    lengths = {}
    nb_singletons = 0
    for pass2_clusterID in pass2_clusters.keys():
        tmp_cluster = []
        for pass1_clusterID in pass2_clusters[pass2_clusterID]:
            tmp_cluster.extend(pass1_clusters[pass1_clusterID])
        tmp_clusters[pass2_clusterID] = tmp_cluster
        lengths[pass2_clusterID] = len(tmp_cluster)
        if len(tmp_cluster) == 1:
            nb_singletons += 1

    for tmp_clusterID in tmp_clusters.keys():
        tmp_cluster = tmp_clusters[tmp_clusterID]
        if len(tmp_cluster) > 1:
            for node in tmp_cluster:
                if node_attributes[node] != 'Clustered_pass1':
                    node_attributes[node] = 'Clustered_pass2'

    if nb_singletons > 0:
        clusters = {'Clustering_singletons': []}
    else:
        clusters = {}

    x = 0
    z = len(str(len(tmp_clusters) - nb_singletons))
    for tmp_clusterID in sorted(
            lengths.keys(), key=lengths.__getitem__, reverse=True):
        tmp_cluster = tmp_clusters[tmp_clusterID]
        if lengths[tmp_clusterID] > 1:
            x += 1
            clusters['Cluster_' + str(x).zfill(z)] = tmp_cluster
        else:
            clusters['Clustering_singletons'].extend(tmp_cluster)

    report['clusters'] = len(tmp_clusters) - nb_singletons
    report['clustering_singletons'] = nb_singletons

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return clusters, sequence_edges, node_attributes, report


def clean_edges(edges):

    final_edges = {}

    for source_seqID in edges.keys():
        for target_seqID in edges[source_seqID].keys():
            edge_data = edges[source_seqID][target_seqID]

            if source_seqID != target_seqID:
                pair = [source_seqID, target_seqID]
                pair.sort()
                seqID1, seqID2 = pair[0], pair[1]
                if seqID1 not in final_edges.keys():
                    final_edges[seqID1] = {seqID2: edge_data}
                else:
                    final_edges[seqID1][seqID2] = edge_data

    return final_edges


def prepare_clusters_info(clusters, proteins, edges, seqIDs, samples,
                          node_attributes):

    edges = clean_edges(edges)

    all_samples = list(set(list(samples.values())))
    all_samples.sort()

    clusters_table = {}
    clusters_stats = {}
    cytoscape_edges = []
    cytoscape_attributes = []

    clusters_IDs = list(clusters.keys())
    clusters_IDs.sort()

    for cluster_ID in clusters_IDs:

        cluster = clusters[cluster_ID]
        cluster.sort()

        nb_nodes = len(cluster)
        nb_nodes_pass1 = 0
        nb_nodes_pass2 = 0

        nb_edges = 0
        nb_edges_intra = 0
        nb_edges_inter = 0

        nb_edges_pass1 = 0
        nb_edges_pass2 = 0

        samples_cluster = {}
        kappa_scores = []
        kappa_scores_intra, kappa_scores_inter = [], []
        identities = []
        coverages = []
        persistences = []
        protein_lengths = []
        clusters_table[cluster_ID] = []

        for node in cluster:

            node_real_ID = seqIDs[node]

            node_sample = samples[node]
            if node_sample in samples_cluster.keys():
                samples_cluster[node_sample] += 1
            else:
                samples_cluster[node_sample] = 1

            node_attribute = node_attributes[node]
            if node_attribute == 'Clustered_pass1':
                nb_nodes_pass1 += 1
            elif node_attribute == 'Clustered_pass2':
                nb_nodes_pass2 += 1

            protein_lengths.append(len(proteins[node]))

            cytoscape_attributes.append(
                [node_real_ID, cluster_ID, node_sample, node_attribute])

            if cluster_ID != 'Clustering_singletons' and node in edges:
                neighbor_nodes = list(edges[node].keys())

                for neighbor_node in neighbor_nodes:
                    data = edges[node][neighbor_node]
                    nb_edges += 1

                    edge_type = data[0]
                    if edge_type == 'Pass1_edge':
                        nb_edges_pass1 += 1
                    elif edge_type == 'Pass2_edge':
                        nb_edges_pass2 += 1

                    kappa_score = data[1]
                    kappa_scores.append(kappa_score)
                    neighbor_node_sample = samples[neighbor_node]
                    if node_sample == neighbor_node_sample:
                        nb_edges_intra += 1
                        kappa_scores_intra.append(kappa_score)
                    else:
                        nb_edges_inter += 1
                        kappa_scores_inter.append(kappa_score)

                    coverage = data[2]
                    coverages.append(coverage)

                    identity = data[3]
                    identities.append(identity)

                    persistence = data[4]
                    persistences.append(persistence)

                    neighbor_node_real_ID = seqIDs[neighbor_node]
                    table_row = [
                        cluster_ID, node_real_ID, neighbor_node_real_ID,
                        edge_type
                    ]
                    table_row.extend(
                        [str(round(i, 4)).ljust(6, '0') for i in data[1:]])
                    clusters_table[cluster_ID].append(table_row)
                    cytoscape_edges.append(table_row[1:])

        clusters_stats[cluster_ID] = [
            nb_nodes, nb_edges, nb_nodes_pass1, nb_edges_pass1, nb_nodes_pass2,
            nb_edges_pass2
        ]

        nb_samples = len(samples_cluster)
        clusters_stats[cluster_ID].append(nb_samples)

        for sample in all_samples:
            if sample in samples_cluster.keys():
                clusters_stats[cluster_ID].append(samples_cluster[sample])
            else:
                clusters_stats[cluster_ID].append(0)

        if cluster_ID != 'Clustering_singletons':
            max_edges_total = (nb_nodes * (nb_nodes - 1)) / 2
            max_edges_intra = 0
            for sample in samples_cluster.keys():
                nb_nodes_sample = samples_cluster[sample]
                max_edges_intra += (nb_nodes_sample *
                                    (nb_nodes_sample - 1)) / 2
            max_edges_inter = max_edges_total - max_edges_intra

            density = round(nb_edges / max_edges_total, 2)

            if max_edges_intra > 0:
                density_intra = round(nb_edges_intra / max_edges_intra, 2)
            else:
                density_intra = 'NA'

            if max_edges_inter > 0:
                density_inter = round(nb_edges_inter / max_edges_inter, 2)
            else:
                density_inter = 'NA'

            mean_prot_length, sd_prot_length = mean(protein_lengths), stdev(
                protein_lengths)
            mean_kappa_score, sd_kappa_score = mean(kappa_scores), stdev(
                kappa_scores)
            mean_kappa_score_intra = mean(kappa_scores_intra)
            mean_kappa_score_inter = mean(kappa_scores_inter)
            mean_identity, sd_identity = mean(identities), stdev(identities)
            mean_coverage, sd_coverage = mean(coverages), stdev(coverages)
            mean_persistence, sd_persistence = mean(persistences), stdev(
                persistences)
            clusters_stats[cluster_ID].extend([
                density, density_intra, density_inter, mean_prot_length,
                sd_prot_length, mean_kappa_score, sd_kappa_score,
                mean_kappa_score_intra, mean_kappa_score_inter, mean_identity,
                sd_identity, mean_coverage, sd_coverage, mean_persistence,
                sd_persistence
            ])

    return (clusters_table, clusters_stats,
            cytoscape_edges, cytoscape_attributes)


def export_clusters_info(clusters_table, clusters_stats, file_id, report,
                         optimization):

    all_samples = list(set(list(samples.values())))
    all_samples.sort()

    print_stdout(
        'Writing clustering reports...',
        level=0,
        end='',
        condition=not optimization)

    clusters_IDs = list(clusters_table.keys())
    clusters_IDs.sort()

    table_file_name = 'Clusters_Table_' + file_id + '.txt'
    table_file = open(table_file_name, 'w')
    table_csv = csv.writer(table_file, dialect='excel-tab')

    table_header = [
        'Cluster ID', 'Sequence 1', 'Sequence 2', 'Edge type', 'KAPPA score',
        'Coverage', 'Identity', 'Persistence'
    ]
    table_csv.writerow(table_header)

    stats_file_name = 'Clusters_Stats_' + file_id + '.txt'
    stats_file = open(stats_file_name, 'w')
    stats_csv = csv.writer(stats_file, dialect='excel-tab')

    stats_header = [
        'Cluster ID', 'Nb nodes', 'Nb edges', 'Nb nodes Pass1',
        'Nb edges Pass1', 'Nb nodes Pass2', 'Nb edges Pass2', 'Nb samples'
    ]
    stats_header.extend(all_samples)
    stats_header.extend([
        'Density', 'Density intra', 'Density inter', 'Mean prot length',
        'SD Prot length', 'Mean k-score', 'SD k-score', 'Mean k-score intra',
        'Mean k-score inter', 'Mean Identity', 'SD Identity', 'Mean Coverage',
        'SD Coverage', 'Mean Persistence', 'SD Persistence'
    ])
    stats_csv.writerow(stats_header)

    proteins_per_cluster = []
    edges_per_cluster = []
    samples_per_cluster = []
    cluster_densities = []
    cluster_densities_intra = []
    cluster_densities_inter = []
    cluster_prot_lengths = []
    cluster_kappa_scores = []
    cluster_kappa_scores_intra = []
    cluster_kappa_scores_inter = []
    cluster_identities = []
    cluster_coverages = []
    cluster_persistences = []

    for cluster_ID in clusters_IDs:
        for table_row in clusters_table[cluster_ID]:
            table_csv.writerow(table_row)

        stats_row = [cluster_ID]
        stats_row.extend(clusters_stats[cluster_ID])
        stats_csv.writerow(stats_row)

        if cluster_ID != 'Clustering_singletons':
            proteins_per_cluster.append(clusters_stats[cluster_ID][0])
            edges_per_cluster.append(clusters_stats[cluster_ID][1])
            samples_per_cluster.append(clusters_stats[cluster_ID][6])
            cluster_densities.append(clusters_stats[cluster_ID][-15])
            cluster_densities_intra.append(clusters_stats[cluster_ID][-14])
            cluster_densities_inter.append(clusters_stats[cluster_ID][-13])
            cluster_prot_lengths.append(clusters_stats[cluster_ID][-12])
            cluster_kappa_scores.append(clusters_stats[cluster_ID][-10])
            cluster_kappa_scores_intra.append(clusters_stats[cluster_ID][-8])
            cluster_kappa_scores_inter.append(clusters_stats[cluster_ID][-7])
            cluster_identities.append(clusters_stats[cluster_ID][-6])
            cluster_coverages.append(clusters_stats[cluster_ID][-4])
            cluster_persistences.append(clusters_stats[cluster_ID][-2])

    table_file.close()
    stats_file.close()

    report['proteins_per_cluster_mean'] = mean(proteins_per_cluster)
    report['proteins_per_cluster_sd'] = stdev(proteins_per_cluster)
    report['proteins_per_cluster_min'] = mini(proteins_per_cluster)
    report['proteins_per_cluster_max'] = maxi(proteins_per_cluster)
    report['edges_per_cluster_mean'] = mean(edges_per_cluster)
    report['edges_per_cluster_sd'] = stdev(edges_per_cluster)
    report['edges_per_cluster_min'] = mini(edges_per_cluster)
    report['edges_per_cluster_max'] = maxi(edges_per_cluster)
    report['samples_per_cluster_mean'] = mean(samples_per_cluster)
    report['samples_per_cluster_sd'] = stdev(samples_per_cluster)
    report['samples_per_cluster_min'] = mini(samples_per_cluster)
    report['samples_per_cluster_max'] = maxi(samples_per_cluster)
    report['cluster_densities_mean'] = mean(cluster_densities)
    report['cluster_densities_sd'] = stdev(cluster_densities)
    report['cluster_densities_min'] = mini(cluster_densities)
    report['cluster_densities_max'] = maxi(cluster_densities)
    report['cluster_densities_intra_mean'] = mean(cluster_densities_intra)
    report['cluster_densities_intra_sd'] = stdev(cluster_densities_intra)
    report['cluster_densities_intra_min'] = mini(cluster_densities_intra)
    report['cluster_densities_intra_max'] = maxi(cluster_densities_intra)
    report['cluster_densities_inter_mean'] = mean(cluster_densities_inter)
    report['cluster_densities_inter_sd'] = stdev(cluster_densities_inter)
    report['cluster_densities_inter_min'] = mini(cluster_densities_inter)
    report['cluster_densities_inter_max'] = maxi(cluster_densities_inter)
    report['cluster_prot_lengths_mean'] = mean(cluster_prot_lengths)
    report['cluster_prot_lengths_sd'] = stdev(cluster_prot_lengths)
    report['cluster_prot_lengths_min'] = mini(cluster_prot_lengths)
    report['cluster_prot_lengths_max'] = maxi(cluster_prot_lengths)
    report['cluster_kappa_scores_mean'] = mean(cluster_kappa_scores)
    report['cluster_kappa_scores_sd'] = stdev(cluster_kappa_scores)
    report['cluster_kappa_scores_min'] = mini(cluster_kappa_scores)
    report['cluster_kappa_scores_max'] = maxi(cluster_kappa_scores)
    report['cluster_kappa_scores_intra_mean'] = mean(
        cluster_kappa_scores_intra)
    report['cluster_kappa_scores_intra_sd'] = stdev(cluster_kappa_scores_intra)
    report['cluster_kappa_scores_intra_min'] = mini(cluster_kappa_scores_intra)
    report['cluster_kappa_scores_intra_max'] = maxi(cluster_kappa_scores_intra)
    report['cluster_kappa_scores_inter_mean'] = mean(
        cluster_kappa_scores_inter)
    report['cluster_kappa_scores_inter_sd'] = stdev(cluster_kappa_scores_inter)
    report['cluster_kappa_scores_inter_min'] = mini(cluster_kappa_scores_inter)
    report['cluster_kappa_scores_inter_max'] = maxi(cluster_kappa_scores_inter)
    report['cluster_identities_mean'] = mean(cluster_identities)
    report['cluster_identities_sd'] = stdev(cluster_identities)
    report['cluster_identities_min'] = mini(cluster_identities)
    report['cluster_identities_max'] = maxi(cluster_identities)
    report['cluster_coverages_mean'] = mean(cluster_coverages)
    report['cluster_coverages_sd'] = stdev(cluster_coverages)
    report['cluster_coverages_min'] = mini(cluster_coverages)
    report['cluster_coverages_max'] = maxi(cluster_coverages)
    report['cluster_persistences_mean'] = mean(cluster_persistences)
    report['cluster_persistences_sd'] = stdev(cluster_persistences)
    report['cluster_persistences_min'] = mini(cluster_persistences)
    report['cluster_persistences_max'] = maxi(cluster_persistences)

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return report


def export_clusters_cytoscape(cytoscape_edges, cytoscape_attributes, file_id,
                              optimization):

    print_stdout(
        'Writing clusters Cytoscape input files...',
        level=0,
        end='',
        condition=not optimization)

    cytoscape_edges_file_name = 'Cytoscape_Clusters_' + file_id + '.txt'
    cytoscape_edges_file = open(cytoscape_edges_file_name, 'w')
    cytoscape_edges_table = csv.writer(
        cytoscape_edges_file, dialect='excel-tab')
    cytoscape_edges_header = [
        'Protein1', 'Protein2', 'Edge_type', 'KAPPA_score', 'Coverage',
        'Identity', 'Persistence'
    ]
    cytoscape_edges_table.writerow(cytoscape_edges_header)
    for row in cytoscape_edges:
        cytoscape_edges_table.writerow(row)
    cytoscape_edges_file.close()

    cytoscape_attributes_file_name = ('Cytoscape_Clusters_attributes_' +
                                      file_id + '.txt')
    cytoscape_attributes_file = open(cytoscape_attributes_file_name, 'w')
    cytoscape_attributes_table = csv.writer(
        cytoscape_attributes_file, dialect='excel-tab')
    cytoscape_attributes_header = ['Protein', 'Cluster', 'Sample', 'Node_type']
    cytoscape_attributes_table.writerow(cytoscape_attributes_header)
    for row in cytoscape_attributes:
        cytoscape_attributes_table.writerow(row)
    cytoscape_attributes_file.close()

    print_stdout('Done.', level=1, end='\n', condition=not optimization)


def export_clusters_fasta(groups, target_proteins, fasta_headers, file_id,
                          report, optimization):

    print_stdout(
        'Exporting clusters as FASTA files...',
        level=0,
        end='',
        condition=not optimization)

    clusters_directory = 'Clusters_' + file_id
    os.mkdir(clusters_directory)
    clusters_files = []

    nb_fasta = 0

    for group_ID in groups:

        group = groups[group_ID]

        nb_fasta += 1

        fasta_file_name = group_ID + '.fasta'
        fasta_file_path = os.path.join(clusters_directory, fasta_file_name)
        clusters_files.append(fasta_file_path)
        fasta_file = open(fasta_file_path, 'w')

        for seqID in group:
            fasta_line = ('>' + fasta_headers[seqID] + '\n' +
                          target_proteins[seqID] + '\n')
            fasta_file.write(fasta_line)

        fasta_file.close()

    print_stdout(
        'Done. ' + str(nb_fasta) + ' clusters FASTA files created.',
        level=1,
        end='\n',
        condition=not optimization)

    return clusters_files, report


def make_groups_part(mapping_scores_part, score_thr, queue):

    groups = {}

    for query_pattern_ID in mapping_scores_part.keys():
        for target_seqID in mapping_scores_part[query_pattern_ID].keys():
            if query_pattern_ID != target_seqID:
                score = mapping_scores_part[query_pattern_ID][target_seqID][0]
                if score >= score_thr:
                    if query_pattern_ID not in groups.keys():
                        groups[query_pattern_ID] = {
                            target_seqID:
                            mapping_scores_part[query_pattern_ID][target_seqID]
                        }
                    else:
                        groups[query_pattern_ID][
                            target_seqID] = mapping_scores_part[
                                query_pattern_ID][target_seqID]

    queue.put(groups)


def make_groups(mapping_scores, threads, score_thr, report, optimization):

    print_stdout(
        'Grouping sequences...', level=0, end='', condition=not optimization)

    mapping_scores_parts = partition_dict(mapping_scores, threads)

    queue = multiprocessing.Queue()
    processes = []

    for mapping_scores_part in mapping_scores_parts:

        process = multiprocessing.Process(
            target=make_groups_part,
            args=(mapping_scores_part, score_thr, queue))
        process.start()
        processes.append(process)

    groups_parts = []
    for p in range(len(processes)):
        groups_parts.append(queue.get())

    groups = {
        query_pattern_ID: {}
        for query_pattern_ID in mapping_scores.keys()
    }

    for groups_part in groups_parts:
        for query_pattern_ID in groups_part.keys():
            for target_seqID in groups_part[query_pattern_ID].keys():
                groups[query_pattern_ID][target_seqID] = groups_part[
                    query_pattern_ID][target_seqID]

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return groups


def prepare_groups_info(groups, proteins, seqIDs, samples):

    all_samples = list(set(list(samples.values())))
    all_samples.sort()

    groups_table = {}
    groups_stats = {}

    for query_pattern_ID in sorted(groups.keys()):

        group = groups[query_pattern_ID]

        nb_proteins = len(group)
        groups_table[query_pattern_ID] = []
        samples_group = {}

        if nb_proteins > 0:

            kappa_scores = []
            identities = []
            coverages = []
            persistences = []
            protein_lengths = []

            for seqID in sorted(group.keys()):

                real_seqID = seqIDs[seqID]
                seqID_sample = samples[seqID]
                if seqID_sample in samples_group.keys():
                    samples_group[seqID_sample] += 1
                else:
                    samples_group[seqID_sample] = 1

                protein_lengths.append(len(proteins[seqID]))
                data = groups[query_pattern_ID][seqID]

                kappa_scores.append(data[0])
                coverages.append(data[1])
                identities.append(data[2])
                persistences.append(data[3])
                table_row = [query_pattern_ID, real_seqID]
                table_row.extend(
                    [str(round(i, 4)).ljust(6, '0') for i in data])
                groups_table[query_pattern_ID].append(table_row)

        else:
            table_row = [query_pattern_ID, 'No hit found']
            groups_table[query_pattern_ID].append(table_row)

        groups_stats[query_pattern_ID] = [nb_proteins]

        nb_samples = len(samples_group)
        groups_stats[query_pattern_ID].append(nb_samples)

        for sample in all_samples:
            if sample in samples_group.keys():
                groups_stats[query_pattern_ID].append(samples_group[sample])
            else:
                groups_stats[query_pattern_ID].append(0)

        if nb_proteins > 0:
            mean_prot_length, sd_prot_length = mean(protein_lengths), stdev(
                protein_lengths)
            mean_kappa_score, sd_kappa_score = mean(kappa_scores), stdev(
                kappa_scores)
            mean_identity, sd_identity = mean(identities), stdev(identities)
            mean_coverage, sd_coverage = mean(coverages), stdev(coverages)
            mean_persistence, sd_persistence = mean(persistences), stdev(
                persistences)
            groups_stats[query_pattern_ID].extend([
                mean_prot_length, sd_prot_length, mean_kappa_score,
                sd_kappa_score, mean_identity, sd_identity, mean_coverage,
                sd_coverage, mean_persistence, sd_persistence
            ])

    return groups_table, groups_stats


def export_groups_info(groups_table, groups_stats, file_id, report,
                       optimization):

    all_samples = list(set(list(samples.values())))
    all_samples.sort()

    print_stdout(
        'Writing grouping reports...',
        level=0,
        end='',
        condition=not optimization)

    groups_IDs = list(groups_table.keys())
    groups_IDs.sort()

    table_file_name = 'Groups_Table_' + file_id + '.txt'
    table_file = open(table_file_name, 'w')
    table_csv = csv.writer(table_file, dialect='excel-tab')

    table_header = [
        'Query pattern', 'Target protein', 'KAPPA score', 'Coverage',
        'Identity', 'Persistence'
    ]
    table_csv.writerow(table_header)

    stats_file_name = 'Groups_Stats_' + file_id + '.txt'
    stats_file = open(stats_file_name, 'w')
    stats_csv = csv.writer(stats_file, dialect='excel-tab')

    stats_header = ['Query pattern', 'Nb target proteins', 'Nb samples']
    stats_header.extend(all_samples)
    stats_header.extend([
        'Mean prot length', 'SD Prot length', 'Mean k-score', 'SD k-score',
        'Mean Identity', 'SD Identity', 'Mean Coverage', 'SD Coverage',
        'Mean Persistence', 'SD Persistence'
    ])
    stats_csv.writerow(stats_header)

    proteins_per_group = []
    samples_per_group = []
    group_prot_lengths = []
    group_kappa_scores = []
    group_identities = []
    group_coverages = []
    group_persistences = []

    report['groups'] = len(groups)
    report['nohit'] = 0

    for group_ID in groups_IDs:
        for table_row in groups_table[group_ID]:
            table_csv.writerow(table_row)

        stats_row = [group_ID]
        stats_row.extend(groups_stats[group_ID])
        stats_csv.writerow(stats_row)

        if groups_stats[group_ID][0] > 0:
            proteins_per_group.append(groups_stats[group_ID][0])
            samples_per_group.append(groups_stats[group_ID][1])
            group_prot_lengths.append(groups_stats[group_ID][-10])
            group_kappa_scores.append(groups_stats[group_ID][-8])
            group_identities.append(groups_stats[group_ID][-6])
            group_coverages.append(groups_stats[group_ID][-4])
            group_persistences.append(groups_stats[group_ID][-2])
        else:
            report['groups'] -= 1
            report['nohit'] += 1

    table_file.close()
    stats_file.close()

    report['proteins_per_group_mean'] = mean(proteins_per_group)
    report['proteins_per_group_sd'] = stdev(proteins_per_group)
    report['proteins_per_group_min'] = mini(proteins_per_group)
    report['proteins_per_group_max'] = maxi(proteins_per_group)
    report['samples_per_group_mean'] = mean(samples_per_group)
    report['samples_per_group_sd'] = stdev(samples_per_group)
    report['samples_per_group_min'] = mini(samples_per_group)
    report['samples_per_group_max'] = maxi(samples_per_group)
    report['group_prot_lengths_mean'] = mean(group_prot_lengths)
    report['group_prot_lengths_sd'] = stdev(group_prot_lengths)
    report['group_prot_lengths_min'] = mini(group_prot_lengths)
    report['group_prot_lengths_max'] = maxi(group_prot_lengths)
    report['group_kappa_scores_mean'] = mean(group_kappa_scores)
    report['group_kappa_scores_sd'] = stdev(group_kappa_scores)
    report['group_kappa_scores_min'] = mini(group_kappa_scores)
    report['group_kappa_scores_max'] = maxi(group_kappa_scores)
    report['group_identities_mean'] = mean(group_identities)
    report['group_identities_sd'] = stdev(group_identities)
    report['group_identities_min'] = mini(group_identities)
    report['group_identities_max'] = maxi(group_identities)
    report['group_coverages_mean'] = mean(group_coverages)
    report['group_coverages_sd'] = stdev(group_coverages)
    report['group_coverages_min'] = mini(group_coverages)
    report['group_coverages_max'] = maxi(group_coverages)
    report['group_persistences_mean'] = mean(group_persistences)
    report['group_persistences_sd'] = stdev(group_persistences)
    report['group_persistences_min'] = mini(group_persistences)
    report['group_persistences_max'] = maxi(group_persistences)

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return report


def export_groups_fasta(groups, target_proteins, fasta_headers, file_id,
                        optimization):

    groups_files = []
    exportable_groups = []
    for query_pattern_ID in groups.keys():
        if len(groups[query_pattern_ID]) > 0:
            exportable_groups.append(query_pattern_ID)

    if len(exportable_groups) == 0:
        print_stdout(
            'WARNING: No sequence found for any query pattern',
            level=0,
            end='\n',
            condition=not optimization)

    else:
        print_stdout(
            'Exporting groups as FASTA files...',
            level=0,
            end='',
            condition=not optimization)

        groups_directory = 'Groups_' + file_id
        os.mkdir(groups_directory)

        nb_fasta = 0

        for query_pattern_ID in exportable_groups:

            group = groups[query_pattern_ID]
            if len(group) > 0:

                nb_fasta += 1

                fasta_file_name = query_pattern_ID + '.fasta'
                fasta_file_path = os.path.join(groups_directory,
                                               fasta_file_name)
                groups_files.append(fasta_file_path)
                fasta_file = open(fasta_file_path, 'w')

                for seqID in group:
                    fasta_line = ('>' + fasta_headers[seqID] + '\n' +
                                  target_proteins[seqID] + '\n')
                    fasta_file.write(fasta_line)

                fasta_file.close()

        print_stdout(
            'Done. ' + str(nb_fasta) + ' groups FASTA files created.',
            level=1,
            end='\n',
            condition=not optimization)

    return groups_files


def import_clusters_fasta(cluster_file_path):

    cluster_file = open(cluster_file_path, 'r')

    proteins = {}
    seqIDs = {}
    samples = {}
    fasta_headers = {}

    i = -1
    prot = 0

    #fasta_structure = re.compile('^>[^\| ]*\|([^\| ]*)[^\[\]]*\[(.*)\].*$')
    fasta_structure = re.compile('^>([^ ]*)[^\[\]]*\[([^\]]*)\].*$')
    for line in cluster_file:
        line = line.strip()

        if line != '':

            if line[0] == '>':
                fasta_ID = fasta_structure.match(line)

                if fasta_ID is not None:
                    i += 1
                    prot += 1
                    simple_ID = 'prot' + str(i)
                    seqIDs[simple_ID] = fasta_ID.group(1)
                    samples[simple_ID] = fasta_ID.group(2)
                    fasta_headers[simple_ID] = line[1:]
                    proteins[simple_ID] = ''

                else:
                    sys.stderr.write('ERROR: File ' + cluster_file_path +
                                     ' does not respect FASTA format.\n')
                    sys.stderr.write('       Sequence identification lines '
                                     'should look like:\n')
                    sys.stderr.write('       >database|sequence_'
                                     'identification [species/sample] '
                                     'description\n\n')
                    sys.exit(1)

            else:
                if i > -1:
                    proteins[simple_ID] += line.upper()

    cluster_file.close()

    cluster_tmp_name = 'tmp_' + os.path.split(cluster_file_path)[-1]
    cluster_tmp = open(cluster_tmp_name, 'w')
    for seqID in proteins:
        record = '>' + seqID + '\n' + proteins[seqID] + '\n'
        cluster_tmp.write(record)
    cluster_tmp.close()

    return proteins, seqIDs, samples, fasta_headers


def blast_clusters(cluster_file_path, makeblastdb_path, blastp_path,
                   blastp_options, mbd_logfile_name, mbd_logfile,
                   blastp_logfile_name, blastp_logfile):

    cluster_tmp_name = 'tmp_' + os.path.split(cluster_file_path)[-1]

    makeblastdb_cmd = [
        makeblastdb_path, '-dbtype', 'prot', '-in', cluster_tmp_name, '-out',
        cluster_tmp_name, '-title', cluster_tmp_name, '-parse_seqids'
    ]
    makeblastdb_process = subprocess.call(
        makeblastdb_cmd, stdout=mbd_logfile, stderr=mbd_logfile)
    if makeblastdb_process != 0:
        sys.stderr.write('ERROR: BLAST database could not be built.\n')
        sys.stderr.write('See file ' + mbd_logfile_name + ' for details.\n\n')
        sys.exit(1)

    blastp_cmd = [
        blastp_path, '-query', cluster_tmp_name, '-db', cluster_tmp_name,
        '-outfmt', '5', '-out', cluster_tmp_name + '.xml'
    ]
    blastp_cmd.extend(blastp_options)
    blastp_process = subprocess.call(
        blastp_cmd, stdout=blastp_logfile, stderr=blastp_logfile)
    if blastp_process != 0:
        sys.stderr.write('ERROR: BLASTp could not be performed.\n')
        sys.stderr.write('See file ' + blastp_logfile_name +
                         ' for details.\n\n')
        sys.exit(1)

    blastdb_extensions = [
        '.phd', '.phi', '.phr', '.pin', '.pnd', '.pni', '.pog', '.ppd', '.ppi',
        '.psd', '.psi', '.psq']
    for ext in blastdb_extensions:
        if os.path.exists(cluster_tmp_name + ext):
            os.remove(cluster_tmp_name + ext)

    os.remove(cluster_tmp_name)


def connect_edges(cluster_file_path, max_evalue, min_hit_pident,
                  min_shortest_seq_pident, min_hit_ppos, min_shortest_seq_ppos,
                  min_hit_length, reciprocity, min_coverage):

    blast_file_name = 'tmp_' + os.path.split(cluster_file_path)[-1] + '.xml'
    blast_file = open(blast_file_name, 'r')

    seq_pairs = {}
    edges = {}

    previous_query = ''
    best_evalue = 0
    best_hit = ''

    (nhsps, qalign, salign, qident, sident,
        qpos, spos, align_length, nident, npos) = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
    evalue = 1000000000
    qhsps, shsps = [], []

    for line in blast_file:
        line = line.strip()

        if line.startswith('<Iteration_query-def>'):
            qseqid = line[line.index('>') + 1:line.index('</')]
            if ' ' in qseqid:
                qseqid = qseqid[:qseqid.index(' ')]

        elif line.startswith('<Iteration_query-len>'):
            qlen = int(line[line.index('>') + 1:line.index('</')])

        elif line.startswith('<Hit_id>'):
            sseqid = line[line.index('>') + 1:line.index('</')]
            if ' ' in sseqid:
                sseqid = sseqid[:sseqid.index(' ')]

        elif line.startswith('<Hit_len>'):
            slen = int(line[line.index('>') + 1:line.index('</')])

        elif line.startswith('<Hsp_evalue>'):
            hsp_evalue = float(line[line.index('>') + 1:line.index('</')])
            if hsp_evalue == 0.0:
                hsp_evalue = 1e-200
            if hsp_evalue < evalue:
                evalue = hsp_evalue

        elif line.startswith('<Hsp_query-from>'):
            qstart = int(line[line.index('>') + 1:line.index('</')])
        elif line.startswith('<Hsp_query-to>'):
            qend = int(line[line.index('>') + 1:line.index('</')])
            qhsps.append((qstart, qend))

        elif line.startswith('<Hsp_hit-from>'):
            sstart = int(line[line.index('>') + 1:line.index('</')])
        elif line.startswith('<Hsp_hit-to>'):
            send = int(line[line.index('>') + 1:line.index('</')])
            shsps.append((sstart, send))

        elif line.startswith('<Hsp_identity>'):
            nident += int(line[line.index('>') + 1:line.index('</')])

        elif line.startswith('<Hsp_positive>'):
            npos += int(line[line.index('>') + 1:line.index('</')])

        elif line.startswith('<Hsp_align-len>'):
            align_length += int(line[line.index('>') + 1:line.index('</')])

        elif line == '</Hit_hsps>':

            new_query = qseqid != previous_query
            no_self_blast = sseqid != qseqid

            if no_self_blast:

                hit_pident = 1.0 * nident / align_length
                if qlen < slen:
                    shortest_seq_pident = 1.0 * nident / qlen
                else:
                    shortest_seq_pident = 1.0 * nident / slen

                hit_ppos = 1.0 * npos / align_length
                if qlen < slen:
                    shortest_seq_ppos = 1.0 * npos / qlen
                else:
                    shortest_seq_ppos = 1.0 * npos / slen

                qidx = [
                    False,
                ] * qlen
                sidx = [
                    False,
                ] * slen

                for hsp in qhsps:
                    for i in range(hsp[0] - 1, hsp[1]):
                        qidx[i] = True
                for hsp in shsps:
                    for i in range(hsp[0] - 1, hsp[1]):
                        sidx[i] = True

                qcov = 1.0 * qidx.count(True) / qlen
                scov = 1.0 * sidx.count(True) / slen

                previous_query = qseqid
                previous_hit = sseqid

                if new_query:
                    best_evalue = evalue
                    in_best_hits = 1
                else:
                    in_best_hits = 0
                    best_evalue_exp = math.log(best_evalue) / math.log(10)
                    evalue_exp = math.log(evalue) / math.log(10)
                    if reciprocity is None or evalue_exp <= best_evalue_exp * (
                            1 - reciprocity):
                        in_best_hits = 1

                if (evalue <= max_evalue and
                        hit_pident >= min_hit_pident and
                        shortest_seq_pident >= min_shortest_seq_pident and
                        hit_ppos >= min_hit_ppos and
                        shortest_seq_ppos >= min_shortest_seq_ppos and
                        align_length >= min_hit_length and
                        qcov >= min_coverage and
                        scov >= min_coverage):

                    if sseqid + '_' + qseqid not in seq_pairs.keys():
                        seq_pairs[qseqid + '_' + sseqid] = [
                            qseqid, sseqid, hit_pident, shortest_seq_pident,
                            hit_ppos, shortest_seq_ppos, evalue, in_best_hits
                        ]
                    else:
                        known_data = seq_pairs[sseqid + '_' + qseqid]
                        if in_best_hits and known_data[7]:
                            if evalue < known_data[6]:
                                new_data = [
                                    hit_pident, shortest_seq_pident, hit_ppos,
                                    shortest_seq_ppos, evalue
                                ]
                            else:
                                new_data = known_data[2:7]

                            if qseqid in edges.keys():
                                edges[qseqid][sseqid] = new_data
                            else:
                                edges[qseqid] = {}
                                edges[qseqid][sseqid] = new_data

                            if sseqid in edges.keys():
                                edges[sseqid][qseqid] = None
                            else:
                                edges[sseqid] = {}
                                edges[sseqid][qseqid] = None

                            seq_pairs.pop(sseqid + '_' + qseqid)
                            if qseqid + '_' + sseqid in seq_pairs:
                                seq_pairs.pop(qseqid + '_' + sseqid)

            (nhsps, qalign, salign, qident, sident, qpos, spos,
                align_length, nident, npos) = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
            evalue = 1000000000
            qhsps, shsps = [], []

    blast_file.close()

    os.remove(blast_file_name)

    return edges


def recursive_search(node, edges, global_network, local_network):

    global_network[node] = 1
    local_network[node] = 1

    neighbor_nodes = list(edges[node].keys())
    for neighbor_node in neighbor_nodes:
        if neighbor_node not in local_network:
            recursive_search(neighbor_node, edges, global_network,
                             local_network)


def build_network(cluster_file_path, proteins, edges):

    cluster_file_name = os.path.split(cluster_file_path)[-1]
    cluster_name = cluster_file_name[:cluster_file_name.rfind('.')]

    nodes = list(edges.keys())
    subclusters = {}

    if edges != {}:
        global_network = {}
        subcluster_nb = 0

        for node in nodes:

            if node not in global_network:

                local_network = {}
                recursive_search(node, edges, global_network, local_network)
                connected_nodes = list(local_network.keys())

                subcluster_nb += 1
                subcluster_ID = cluster_name + '_' + str(subcluster_nb)
                subclusters[subcluster_ID] = connected_nodes

    seqIDs = list(proteins.keys())
    singletons = []

    for seqID in seqIDs:
        if seqID not in nodes:
            singletons.append(seqID)

    if singletons != []:
        subcluster_ID = cluster_name + '_S'
        subclusters[subcluster_ID] = singletons

    return subclusters


def export_subclusters_fasta(subclusters, subclusters_directory, proteins,
                             fasta_headers):

    subclusters_IDs = list(subclusters.keys())
    subclusters_IDs.sort()

    for subcluster_ID in subclusters_IDs:
        subcluster_nodes = subclusters[subcluster_ID]
        subcluster_nodes.sort()
        subcluster_file_name = subcluster_ID + '.fasta'
        subcluster_file_path = os.path.join(subclusters_directory,
                                            subcluster_file_name)
        subcluster_file = open(subcluster_file_path, 'w')

        for node in subcluster_nodes:
            fasta_record = ('>' + fasta_headers[node] + '\n' +
                            proteins[node] + '\n')
            subcluster_file.write(fasta_record)

        subcluster_file.close()


def export_subclusters_info_part(subclusters, proteins, edges, seqIDs, samples,
                                 all_samples):

    subcluster_table = {}
    subcluster_stats = {}
    cytoscape_edges = []
    cytoscape_attributes = []

    subclusters_IDs = list(subclusters.keys())
    subclusters_IDs.sort()

    for subcluster_ID in subclusters_IDs:

        cluster_ID = subcluster_ID[:subcluster_ID.rfind('_')]

        subcluster = subclusters[subcluster_ID]
        subcluster.sort()

        nb_nodes = len(subcluster)
        nb_edges = 0
        nb_edges_intra = 0
        nb_edges_inter = 0

        samples_subcluster = {}
        evalues = []
        hit_pidents = []
        hit_pidents_intra, hit_pidents_inter = [], []
        shortest_seq_pidents = []
        hit_pposs = []
        hit_pposs_intra, hit_pposs_inter = [], []
        shortest_seq_pposs = []
        protein_lengths = []
        subcluster_table[subcluster_ID] = []

        for node in subcluster:

            node_real_ID = seqIDs[node]

            node_sample = samples[node]
            if node_sample in samples_subcluster.keys():
                samples_subcluster[node_sample] += 1
            else:
                samples_subcluster[node_sample] = 1

            protein_lengths.append(len(proteins[node]))

            cytoscape_attributes.append(
                [node_real_ID, cluster_ID, subcluster_ID, node_sample])

            if subcluster_ID.endswith('_S') == False:
                neighbor_nodes = list(edges[node].keys())

                for neighbor_node in neighbor_nodes:
                    data = edges[node][neighbor_node]
                    if data is not None:
                        nb_edges += 1

                        neighbor_node_sample = samples[neighbor_node]
                        if node_sample == neighbor_node_sample:
                            nb_edges_intra += 1
                            hit_pidents_intra.append(data[0])
                            hit_pposs_intra.append(data[2])
                        else:
                            nb_edges_inter += 1
                            hit_pidents_inter.append(data[0])
                            hit_pidents_inter.append(data[2])

                        hit_pidents.append(data[0])
                        shortest_seq_pidents.append(data[1])
                        hit_pposs.append(data[2])
                        shortest_seq_pidents.append(data[3])
                        evalues.append(data[4])

                        neighbor_node_real_ID = seqIDs[neighbor_node]
                        table_row = [
                            cluster_ID, subcluster_ID, node_real_ID,
                            neighbor_node_real_ID,
                            str(round(data[0], 4)).ljust(6, '0'),
                            str(round(data[1], 4)).ljust(6, '0'),
                            str(round(data[2], 4)).ljust(6, '0'),
                            str(round(data[3], 4)).ljust(6, '0'), data[4]
                        ]
                        subcluster_table[subcluster_ID].append(table_row)

                        cytoscape_edges.append([
                            node_real_ID, neighbor_node_real_ID,
                            str(round(data[0], 4)).ljust(6, '0'),
                            str(round(data[1], 4)).ljust(6, '0'),
                            str(round(data[2], 4)).ljust(6, '0'),
                            str(round(data[3], 4)).ljust(6, '0'),
                            round(-1 * math.log(data[4]), 1)
                        ])

            # else:
            # cytoscape_edges.append([node_real_ID])

        subcluster_stats[subcluster_ID] = [
            cluster_ID, subcluster_ID, nb_nodes, nb_edges
        ]

        nb_samples = len(samples_subcluster)
        subcluster_stats[subcluster_ID].append(nb_samples)

        for sample in all_samples:
            if sample in samples_subcluster.keys():
                subcluster_stats[subcluster_ID].append(
                    samples_subcluster[sample])
            else:
                subcluster_stats[subcluster_ID].append(0)

        if subcluster_ID.endswith('_S') is False:
            max_edges_total = (nb_nodes * (nb_nodes - 1)) / 2
            max_edges_intra = 0
            for sample in samples_subcluster.keys():
                nb_nodes_sample = samples_subcluster[sample]
                max_edges_intra += (nb_nodes_sample *
                                    (nb_nodes_sample - 1)) / 2
            max_edges_inter = max_edges_total - max_edges_intra

            density = round(nb_edges / max_edges_total, 2)

            if max_edges_intra > 0:
                density_intra = round(nb_edges_intra / max_edges_intra, 2)
            else:
                density_intra = 'NA'

            if max_edges_inter > 0:
                density_inter = round(nb_edges_inter / max_edges_inter, 2)
            else:
                density_inter = 'NA'

            mean_prot_length, sd_prot_length = mean(protein_lengths), stdev(
                protein_lengths)
            mean_hit_pident, sd_hit_pident = mean(hit_pidents), stdev(
                hit_pidents)
            mean_hit_pident_intra = mean(hit_pidents_intra)
            mean_hit_pident_inter = mean(hit_pidents_inter)
            mean_shortest_seq_pident, sd_shortest_seq_pident = mean(
                shortest_seq_pidents), stdev(shortest_seq_pidents)
            mean_hit_ppos, sd_hit_ppos = mean(hit_pposs), stdev(hit_pposs)
            mean_hit_ppos_intra = mean(hit_pposs_intra)
            mean_hit_ppos_inter = mean(hit_pposs_inter)
            mean_shortest_seq_ppos, sd_shortest_seq_ppos = mean(
                shortest_seq_pposs), stdev(shortest_seq_pposs)
            mean_evalue, sd_evalue = mean(evalues), stdev(evalues)
            subcluster_stats[subcluster_ID].extend([
                density, density_intra, density_inter, mean_prot_length,
                sd_prot_length, mean_hit_pident, sd_hit_pident,
                mean_hit_pident_intra, mean_hit_pident_inter,
                mean_shortest_seq_pident, sd_shortest_seq_pident,
                mean_hit_ppos, sd_hit_ppos, mean_hit_ppos_intra,
                mean_hit_ppos_inter, mean_shortest_seq_ppos,
                sd_shortest_seq_ppos, mean_evalue, sd_evalue
            ])

    return (subcluster_table, subcluster_stats,
            cytoscape_edges, cytoscape_attributes)


def make_subclusters_part(
        clusters_files_part, clusters_files, subclusters_directory, max_evalue,
        min_hit_pident, min_shortest_seq_pident, min_hit_ppos,
        min_shortest_seq_ppos, min_hit_length, reciprocity, min_coverage,
        all_samples, makeblastdb_path, blastp_path, blastp_options,
        mbd_logfile_name, mbd_logfile, blastp_logfile_name, blastp_logfile,
        queue):

    subclusters_tables, subclusters_stats = {}, {}
    cytoscape_edges, cytoscape_attributes = [], []

    for cluster_file_name in clusters_files_part:

        proteins, seqIDs, samples, fasta_headers = import_clusters_fasta(
            cluster_file_name)
        blast_clusters(cluster_file_name, makeblastdb_path, blastp_path,
                       blastp_options, mbd_logfile_name, mbd_logfile,
                       blastp_logfile_name, blastp_logfile)
        edges = connect_edges(cluster_file_name, max_evalue, min_hit_pident,
                              min_shortest_seq_pident, min_hit_ppos,
                              min_shortest_seq_ppos, min_hit_length,
                              reciprocity, min_coverage)
        subclusters = build_network(cluster_file_name, proteins, edges)
        export_subclusters_fasta(subclusters, subclusters_directory, proteins,
                                 fasta_headers)
        (subclusters_tables_part, subclusters_stats_part, cytoscape_edges_part,
            cytoscape_attributes_part) = export_subclusters_info_part(
            subclusters, proteins, edges, seqIDs, samples, all_samples)
        subclusters_tables.update(subclusters_tables_part)
        subclusters_stats.update(subclusters_stats_part)
        cytoscape_edges.extend(cytoscape_edges_part)
        cytoscape_attributes.extend(cytoscape_attributes_part)

    subclusters_info = (subclusters_tables, subclusters_stats, cytoscape_edges,
                        cytoscape_attributes)
    queue.put(subclusters_info)


def make_subclusters(clusters_files, threads, file_id, max_evalue,
                     min_hit_pident, min_shortest_seq_pident, min_hit_ppos,
                     min_shortest_seq_ppos, min_hit_length, reciprocity,
                     min_coverage, makeblastdb_path, blastp_path,
                     blastp_options, report, optimization):

    print_stdout(
        'Subclustering sequences...',
        level=0,
        end='',
        condition=not optimization)

    subclusters_directory = 'Subclusters_' + file_id
    os.mkdir(subclusters_directory)

    clusters_files_parts = partition_list(clusters_files, threads)

    subclusters_info = []
    subclusters_tables, subclusters_stats = {}, {}
    cytoscape_edges, cytoscape_attributes = [], []

    all_samples = []

    nb_clusters = 0
    nb_proteins = 0

    for cluster_file in clusters_files:
        cluster = open(cluster_file, 'r')

        nb_clusters += 1
        fasta_structure = re.compile('^>([^ ]*)[^\[\]]*\[([^\]]*)\].*$')

        for line in cluster:
            line = line.strip()
            if line != '' and line[0] == '>':
                nb_proteins += 1
                fasta_ID = fasta_structure.match(line)
                if fasta_ID is not None:
                    sample = fasta_ID.group(2)
                    if sample not in all_samples:
                        all_samples.append(sample)
                else:
                    print('ERROR: File ' + cluster_file +
                          ' does not respect FASTA format. XXX')
                    print('       Sequence identification lines should '
                          'look like:')
                    print('       >database|sequence_identification '
                          '[species/sample] description\n')
                    sys.exit(1)
        cluster.close()

    mbd_logfile_name = 'Subclustering_' + file_id + '_makeblastdb.log'
    mbd_logfile = open(mbd_logfile_name, 'w')
    blastp_logfile_name = 'Subclustering_' + file_id + '_blastp.log'
    blastp_logfile = open(blastp_logfile_name, 'w')

    queue = multiprocessing.Queue()
    processes = []

    for clusters_files_part in clusters_files_parts:
        process = multiprocessing.Process(
            target=make_subclusters_part,
            args=(clusters_files_part, clusters_files, subclusters_directory,
                  max_evalue, min_hit_pident, min_shortest_seq_pident,
                  min_hit_ppos, min_shortest_seq_ppos, min_hit_length,
                  reciprocity, min_coverage, all_samples, makeblastdb_path,
                  blastp_path, blastp_options, mbd_logfile_name, mbd_logfile,
                  blastp_logfile_name, blastp_logfile, queue))
        process.start()
        processes.append(process)

    for p in range(len(processes)):
        subclusters_info.append(queue.get())

    for subcluster_info in subclusters_info:
        subclusters_tables.update(subcluster_info[0])
        subclusters_stats.update(subcluster_info[1])
        cytoscape_edges.extend(subcluster_info[2])
        cytoscape_attributes.extend(subcluster_info[3])

    mbd_logfile.close()
    blastp_logfile.close()

    report['imported_clusters'] = nb_clusters
    report['imported_proteins_subclustering'] = nb_proteins

    print_stdout(
        'Done. ' + str(len(subclusters_tables)) + ' subclusters created.',
        level=1,
        end='\n',
        condition=not optimization)

    return (subclusters_tables, subclusters_stats, cytoscape_edges,
            cytoscape_attributes, all_samples, report)


def export_subclusters_info(subclusters_tables, subclusters_stats, all_samples,
                            file_id, report, optimization):

    print_stdout(
        'Writing subclustering reports...',
        level=0,
        end='',
        condition=not optimization)

    subclusters_IDs = list(subclusters_tables.keys())
    subclusters_IDs.sort()

    table_file_name = 'Subclusters_Table_' + file_id + '.txt'
    table_file = open(table_file_name, 'w')
    table_csv = csv.writer(table_file, dialect='excel-tab')

    table_header = [
        'Cluster ID', 'Subcluster ID', 'Sequence 1', 'Sequence 2', 'Hit id',
        'Shortest seq id', 'Hit ppos', 'Shortest seq ppos', 'E-value'
    ]
    table_csv.writerow(table_header)

    stats_file_name = 'Subclusters_Stats_' + file_id + '.txt'
    stats_file = open(stats_file_name, 'w')
    stats_csv = csv.writer(stats_file, dialect='excel-tab')

    stats_header = [
        'Cluster ID', 'Subcluster ID', 'Nb nodes', 'Nb edges', 'Nb samples'
    ]
    stats_header.extend(all_samples)
    stats_header.extend([
        'Density', 'Density intra', 'Density inter', 'Mean prot length',
        'SD Prot length', 'Mean Hit id', 'SD Hit id', 'Mean hit id intra',
        'Mean hit id inter', 'Mean Shortest seq id', 'SD Shortest seq id',
        'Mean Hit ppos', 'SD Hit ppos', 'Mean hit ppos intra',
        'Mean hit ppos inter', 'Mean Shortest seq ppos',
        'SD Shortest seq ppos', 'Mean E-value', 'SD E-value'
    ])
    stats_csv.writerow(stats_header)

    nb_subclusters = 0
    nb_singletons_subclusters = 0
    proteins_per_subcluster = []
    edges_per_subcluster = []
    samples_per_subcluster = []
    subcluster_densities = []
    subcluster_densities_intra = []
    subcluster_densities_inter = []
    subcluster_prot_lengths = []
    subcluster_hit_pidents = []
    subcluster_hit_pidents_intra = []
    subcluster_hit_pidents_inter = []
    subcluster_hit_pposs = []
    subcluster_hit_pposs_intra = []
    subcluster_hit_pposs_inter = []

    for subcluster_ID in subclusters_IDs:
        for table_row in subclusters_tables[subcluster_ID]:
            table_csv.writerow(table_row)

        stats_row = subclusters_stats[subcluster_ID]
        stats_csv.writerow(stats_row)

        if subcluster_ID.endswith('_S'):
            nb_singletons_subclusters += 1
        else:
            nb_subclusters += 1
            proteins_per_subcluster.append(stats_row[2])
            edges_per_subcluster.append(stats_row[3])
            samples_per_subcluster.append(stats_row[4])
            subcluster_densities.append(stats_row[-19])
            subcluster_densities_intra.append(stats_row[-18])
            subcluster_densities_inter.append(stats_row[-17])
            subcluster_prot_lengths.append(stats_row[-16])
            subcluster_hit_pidents.append(stats_row[-14])
            subcluster_hit_pidents_intra.append(stats_row[-12])
            subcluster_hit_pidents_inter.append(stats_row[-11])
            subcluster_hit_pposs.append(stats_row[-8])
            subcluster_hit_pposs_intra.append(stats_row[-6])
            subcluster_hit_pposs_inter.append(stats_row[-5])

    table_file.close()
    stats_file.close()

    report['nb_subclusters'] = nb_subclusters
    report['nb_singletons_subclusters'] = nb_singletons_subclusters
    report['proteins_per_subcluster_mean'] = mean(proteins_per_subcluster)
    report['proteins_per_subcluster_sd'] = stdev(proteins_per_subcluster)
    report['proteins_per_subcluster_min'] = mini(proteins_per_subcluster)
    report['proteins_per_subcluster_max'] = maxi(proteins_per_subcluster)
    report['edges_per_subcluster_mean'] = mean(edges_per_subcluster)
    report['edges_per_subcluster_sd'] = stdev(edges_per_subcluster)
    report['edges_per_subcluster_min'] = mini(edges_per_subcluster)
    report['edges_per_subcluster_max'] = maxi(edges_per_subcluster)
    report['samples_per_subcluster_mean'] = mean(samples_per_subcluster)
    report['samples_per_subcluster_sd'] = stdev(samples_per_subcluster)
    report['samples_per_subcluster_min'] = mini(samples_per_subcluster)
    report['samples_per_subcluster_max'] = maxi(samples_per_subcluster)
    report['subcluster_densities_mean'] = mean(subcluster_densities)
    report['subcluster_densities_sd'] = stdev(subcluster_densities)
    report['subcluster_densities_min'] = mini(subcluster_densities)
    report['subcluster_densities_max'] = maxi(subcluster_densities)
    report['subcluster_densities_intra_mean'] = mean(
        subcluster_densities_intra)
    report['subcluster_densities_intra_sd'] = stdev(subcluster_densities_intra)
    report['subcluster_densities_intra_min'] = mini(subcluster_densities_intra)
    report['subcluster_densities_intra_max'] = maxi(subcluster_densities_intra)
    report['subcluster_densities_inter_mean'] = mean(
        subcluster_densities_inter)
    report['subcluster_densities_inter_sd'] = stdev(subcluster_densities_inter)
    report['subcluster_densities_inter_min'] = mini(subcluster_densities_inter)
    report['subcluster_densities_inter_max'] = maxi(subcluster_densities_inter)
    report['subcluster_hit_pidents_mean'] = mean(subcluster_hit_pidents)
    report['subcluster_hit_pidents_sd'] = stdev(subcluster_hit_pidents)
    report['subcluster_hit_pidents_min'] = mini(subcluster_hit_pidents)
    report['subcluster_hit_pidents_max'] = maxi(subcluster_hit_pidents)
    report['subcluster_hit_pidents_intra_mean'] = mean(
        subcluster_hit_pidents_intra)
    report['subcluster_hit_pidents_intra_sd'] = stdev(
        subcluster_hit_pidents_intra)
    report['subcluster_hit_pidents_intra_min'] = mini(
        subcluster_hit_pidents_intra)
    report['subcluster_hit_pidents_intra_max'] = maxi(
        subcluster_hit_pidents_intra)
    report['subcluster_hit_pidents_inter_mean'] = mean(
        subcluster_hit_pidents_inter)
    report['subcluster_hit_pidents_inter_sd'] = stdev(
        subcluster_hit_pidents_inter)
    report['subcluster_hit_pidents_inter_min'] = mini(
        subcluster_hit_pidents_inter)
    report['subcluster_hit_pidents_inter_max'] = maxi(
        subcluster_hit_pidents_inter)
    report['subcluster_hit_pposs_mean'] = mean(subcluster_hit_pposs)
    report['subcluster_hit_pposs_sd'] = stdev(subcluster_hit_pposs)
    report['subcluster_hit_pposs_min'] = mini(subcluster_hit_pposs)
    report['subcluster_hit_pposs_max'] = maxi(subcluster_hit_pposs)
    report['subcluster_hit_pposs_intra_mean'] = mean(
        subcluster_hit_pposs_intra)
    report['subcluster_hit_pposs_intra_sd'] = stdev(subcluster_hit_pposs_intra)
    report['subcluster_hit_pposs_intra_min'] = mini(subcluster_hit_pposs_intra)
    report['subcluster_hit_pposs_intra_max'] = maxi(subcluster_hit_pposs_intra)
    report['subcluster_hit_pposs_inter_mean'] = mean(
        subcluster_hit_pposs_inter)
    report['subcluster_hit_pposs_inter_sd'] = stdev(subcluster_hit_pposs_inter)
    report['subcluster_hit_pposs_inter_min'] = mini(subcluster_hit_pposs_inter)
    report['subcluster_hit_pposs_inter_max'] = maxi(subcluster_hit_pposs_inter)
    report['subcluster_prot_lengths_mean'] = mean(subcluster_prot_lengths)
    report['subcluster_prot_lengths_sd'] = stdev(subcluster_prot_lengths)
    report['subcluster_prot_lengths_min'] = mini(subcluster_prot_lengths)
    report['subcluster_prot_lengths_max'] = maxi(subcluster_prot_lengths)

    print_stdout('Done.', level=1, end='\n', condition=not optimization)

    return report


def export_subclusters_cytoscape(cytoscape_edges, cytoscape_attributes,
                                 file_id, optimization):

    print_stdout(
        'Writing subclusters Cytoscape input files...',
        level=0,
        end='',
        condition=not optimization)

    cytoscape_edges_file_name = 'Cytoscape_Subclusters_' + file_id + '.txt'
    cytoscape_edges_file = open(cytoscape_edges_file_name, 'w')
    cytoscape_edges_table = csv.writer(
        cytoscape_edges_file, dialect='excel-tab')
    cytoscape_edges_header = [
        'Protein1', 'Protein2', 'Hit_identity', 'Shortest_seq_identity',
        'Hit_ppositives', 'Shortest_seq_ppositives', '-log(evalue)'
    ]
    cytoscape_edges_table.writerow(cytoscape_edges_header)
    for row in cytoscape_edges:
        cytoscape_edges_table.writerow(row)
    cytoscape_edges_file.close()

    cytoscape_attributes_file_name = ('Cytoscape_Subclusters_attributes_' +
                                      file_id + '.txt')
    cytoscape_attributes_file = open(cytoscape_attributes_file_name, 'w')
    cytoscape_attributes_table = csv.writer(
        cytoscape_attributes_file, dialect='excel-tab')
    cytoscape_attributes_header = ['Protein', 'Subcluster', 'Sample']
    cytoscape_attributes_table.writerow(cytoscape_attributes_header)
    for row in cytoscape_attributes:
        cytoscape_attributes_table.writerow(row)
    cytoscape_attributes_file.close()

    print_stdout('Done.', level=1, end='\n', condition=not optimization)


def write_execution_report(report):

    report_file_name = 'Report_' + report['o'] + '.txt'
    report_file = open(report_file_name, 'w')

    col = 38

    report_file.write('*'*80 + '\n')
    report_file.write('*' + ' '*17 +
                      'KEY AMINOACID PATTERN-BASED PROTEIN ANALYZER' +
                      ' '*17 + '*' + '\n')
    report_file.write('*' + ' '*31 +
                      'Execution report' +
                      ' '*31 + '*' + '\n')
    report_file.write('*'*80 + '\n\n')

    report_file.write(('Started:').ljust(col) +
                      report['start_time'].strftime('%d-%b-%Y  %H:%M:%S') +
                      '\n')
    report_file.write(('Finshed:').ljust(col) +
                      report['stop_time'].strftime('%d-%b-%Y  %H:%M:%S') +
                      '\n')
    report_file.write(('Execution time:').ljust(col) +
                      str(report['execution_time']) + '  (' +
                      str(report['execution_time'].seconds) + ' sec)\n')

    if not report['b']:

        report_file.write('\n\nPATTERN MAPPING RESULTS\n')
        report_file.write('-'*80 + '\n\n')

        report_file.write(('Number of imported proteins:').ljust(col) +
                          str(report['imported_proteins_clustering']) + '\n')
        report_file.write(('Number of rejected proteins:').ljust(col) +
                          str(report['rejected_proteins']) + '\n')
        report_file.write(('Number of retained proteins:').ljust(col) +
                          str(report['retained_proteins']) + '\n')
        report_file.write(('Number of query patterns:').ljust(col) +
                          str(report['query_patterns']) + '\n')

        if report['q'] == []:

            report_file.write('\n\nPATTERN-BASED DE NOVO CLUSTERING RESULTS\n')
            report_file.write('-'*80 + '\n\n')

            report_file.write(('Number of clusters built:').ljust(col) +
                              str(report['clusters']) + '\n')
            report_file.write(('Number of singleton clusters:').ljust(col) +
                              str(report['clustering_singletons']) + '\n\n')

            report_file.write(('').ljust(col) + 'Mean\tSD\tMin\tMax\n')
            report_file.write(('Nb of proteins per cluster:').ljust(col) +
                              str(report['proteins_per_cluster_mean']) + '\t' +
                              str(report['proteins_per_cluster_sd']) + '\t' +
                              str(report['proteins_per_cluster_min']) + '\t' +
                              str(report['proteins_per_cluster_max']) + '\n')
            report_file.write(('Nb of edges per cluster:').ljust(col) +
                              str(report['edges_per_cluster_mean']) + '\t' +
                              str(report['edges_per_cluster_sd']) + '\t' +
                              str(report['edges_per_cluster_min']) + '\t' +
                              str(report['edges_per_cluster_max']) + '\n')

            report_file.write(('Nb of samples per cluster:').ljust(col) +
                              str(report['samples_per_cluster_mean']) + '\t' +
                              str(report['samples_per_cluster_sd']) + '\t' +
                              str(report['samples_per_cluster_min']) + '\t' +
                              str(report['samples_per_cluster_max']) + '\n')
            report_file.write(('Cluster mean protein length:').ljust(col) +
                              str(report['cluster_prot_lengths_mean']) + '\t' +
                              str(report['cluster_prot_lengths_sd']) + '\t' +
                              str(report['cluster_prot_lengths_min']) + '\t' +
                              str(report['cluster_prot_lengths_max']) + '\n')
            report_file.write(('Cluster_density').ljust(col) +
                              str(report['cluster_densities_mean']) + '\t' +
                              str(report['cluster_densities_sd']) + '\t' +
                              str(report['cluster_densities_min']) + '\t' +
                              str(report['cluster_densities_max']) + '\n')
            report_file.write(
                ('  - intra-sample density:').ljust(col) +
                str(report['cluster_densities_intra_mean']) + '\t' +
                str(report['cluster_densities_intra_sd']) + '\t' +
                str(report['cluster_densities_intra_min']) + '\t' +
                str(report['cluster_densities_intra_max']) + '\n')
            report_file.write(
                ('  - inter-sample density:').ljust(col) +
                str(report['cluster_densities_inter_mean']) + '\t' +
                str(report['cluster_densities_inter_sd']) + '\t' +
                str(report['cluster_densities_inter_min']) + '\t' +
                str(report['cluster_densities_inter_max']) + '\n')
            report_file.write(('Cluster mean KAPPA-score:').ljust(col) +
                              str(report['cluster_kappa_scores_mean']) + '\t' +
                              str(report['cluster_kappa_scores_sd']) + '\t' +
                              str(report['cluster_kappa_scores_min']) + '\t' +
                              str(report['cluster_kappa_scores_max']) + '\n')
            report_file.write(
                ('  - intra-sample k-score:').ljust(col) +
                str(report['cluster_kappa_scores_intra_mean']) + '\t' +
                str(report['cluster_kappa_scores_intra_sd']) + '\t' +
                str(report['cluster_kappa_scores_intra_min']) + '\t' +
                str(report['cluster_kappa_scores_intra_max']) + '\n')
            report_file.write(
                ('  - intra-sample k-score:').ljust(col) +
                str(report['cluster_kappa_scores_inter_mean']) + '\t' +
                str(report['cluster_kappa_scores_inter_sd']) + '\t' +
                str(report['cluster_kappa_scores_inter_min']) + '\t' +
                str(report['cluster_kappa_scores_inter_max']) + '\n')
            report_file.write(('Cluster mean identity (I):').ljust(col) +
                              str(report['cluster_identities_mean']) + '\t' +
                              str(report['cluster_identities_sd']) + '\t' +
                              str(report['cluster_identities_min']) + '\t' +
                              str(report['cluster_identities_max']) + '\n')
            report_file.write(('Cluster mean coverage (C):').ljust(col) +
                              str(report['cluster_coverages_mean']) + '\t' +
                              str(report['cluster_coverages_sd']) + '\t' +
                              str(report['cluster_coverages_min']) + '\t' +
                              str(report['cluster_coverages_max']) + '\n')
            report_file.write(('Cluster mean persistence (P):').ljust(col) +
                              str(report['cluster_persistences_mean']) + '\t' +
                              str(report['cluster_persistences_sd']) + '\t' +
                              str(report['cluster_persistences_min']) + '\t' +
                              str(report['cluster_persistences_max']) + '\n')

        else:

            report_file.write('\n\nPATTERN-BASED AB INITIO SEARCH RESULTS\n')
            report_file.write('-'*80 + '\n\n')

            report_file.write(('Number of groups formed:').ljust(col) +
                              str(report['groups']) + '\n')
            report_file.write(('Query patterns with no hit:').ljust(col) +
                              str(report['nohit']) + '\n\n')

            report_file.write(('').ljust(col) + 'Mean\tSD\tMin\tMax\n')
            report_file.write(('Nb of proteins per group:').ljust(col) +
                              str(report['proteins_per_group_mean']) + '\t' +
                              str(report['proteins_per_group_sd']) + '\t' +
                              str(report['proteins_per_group_min']) + '\t' +
                              str(report['proteins_per_group_max']) + '\n')
            report_file.write(('Nb of samples per group:').ljust(col) +
                              str(report['samples_per_group_mean']) + '\t' +
                              str(report['samples_per_group_sd']) + '\t' +
                              str(report['samples_per_group_min']) + '\t' +
                              str(report['samples_per_group_max']) + '\n')
            report_file.write(('Group mean protein length:').ljust(col) +
                              str(report['group_prot_lengths_mean']) + '\t' +
                              str(report['group_prot_lengths_sd']) + '\t' +
                              str(report['group_prot_lengths_min']) + '\t' +
                              str(report['group_prot_lengths_max']) + '\n')
            report_file.write(('Group mean KAPPA-score:').ljust(col) +
                              str(report['group_kappa_scores_mean']) + '\t' +
                              str(report['group_kappa_scores_sd']) + '\t' +
                              str(report['group_kappa_scores_min']) + '\t' +
                              str(report['group_kappa_scores_max']) + '\n')
            report_file.write(('Group mean identity (I):').ljust(col) +
                              str(report['group_identities_mean']) + '\t' +
                              str(report['group_identities_sd']) + '\t' +
                              str(report['group_identities_min']) + '\t' +
                              str(report['group_identities_max']) + '\n')
            report_file.write(('Group mean coverage (C):').ljust(col) +
                              str(report['group_coverages_mean']) + '\t' +
                              str(report['group_coverages_sd']) + '\t' +
                              str(report['group_coverages_min']) + '\t' +
                              str(report['group_coverages_max']) + '\n')
            report_file.write(('Group mean persistence (P):').ljust(col) +
                              str(report['group_persistences_mean']) + '\t' +
                              str(report['group_persistences_sd']) + '\t' +
                              str(report['group_persistences_min']) + '\t' +
                              str(report['group_persistences_max']) + '\n')

    if report['s']:
        report_file.write('\n\nBLAST-BASED SUBCLUSTERING RESULTS\n')
        report_file.write('-'*80 + '\n\n')

        report_file.write(('Number of imported clusters:').ljust(col) +
                          str(report['imported_clusters']) + '\n')
        report_file.write(('Number of imported proteins:').ljust(col) +
                          str(report['imported_proteins_subclustering']) +
                          '\n\n')

        report_file.write(('Number of subclusters built:').ljust(col) +
                          str(report['nb_subclusters']) + '\n')
        report_file.write(('Number of singleton subclusters:').ljust(col) +
                          str(report['nb_singletons_subclusters']) + '\n\n')

        report_file.write(('').ljust(col) + 'Mean\tSD\tMin\tMax\n')

        report_file.write(('Nb of proteins per subcluster:').ljust(col) +
                          str(report['proteins_per_subcluster_mean']) + '\t' +
                          str(report['proteins_per_subcluster_sd']) + '\t' +
                          str(report['proteins_per_subcluster_min']) + '\t' +
                          str(report['proteins_per_subcluster_max']) + '\n')
        report_file.write(('Nb of edges per subcluster:').ljust(col) +
                          str(report['edges_per_subcluster_mean']) + '\t' +
                          str(report['edges_per_subcluster_sd']) + '\t' +
                          str(report['edges_per_subcluster_min']) + '\t' +
                          str(report['edges_per_subcluster_max']) + '\n')
        report_file.write(('Nb of samples per subcluster:').ljust(col) +
                          str(report['samples_per_subcluster_mean']) + '\t' +
                          str(report['samples_per_subcluster_sd']) + '\t' +
                          str(report['samples_per_subcluster_min']) + '\t' +
                          str(report['samples_per_subcluster_max']) + '\n')
        report_file.write(('Subcluster density:').ljust(col) +
                          str(report['subcluster_densities_mean']) + '\t' +
                          str(report['subcluster_densities_sd']) + '\t' +
                          str(report['subcluster_densities_min']) + '\t' +
                          str(report['subcluster_densities_max']) + '\n')
        report_file.write(
            ('  - intra-sample density:').ljust(col) +
            str(report['subcluster_densities_intra_mean']) + '\t' +
            str(report['subcluster_densities_intra_sd']) + '\t' +
            str(report['subcluster_densities_intra_min']) + '\t' +
            str(report['subcluster_densities_intra_max']) + '\n')
        report_file.write(
            ('  - inter-sample density:').ljust(col) +
            str(report['subcluster_densities_inter_mean']) + '\t' +
            str(report['subcluster_densities_inter_sd']) + '\t' +
            str(report['subcluster_densities_inter_min']) + '\t' +
            str(report['subcluster_densities_inter_max']) + '\n')
        report_file.write(('Subcluster mean hit identity:').ljust(col) +
                          str(report['subcluster_hit_pidents_mean']) + '\t' +
                          str(report['subcluster_hit_pidents_sd']) + '\t' +
                          str(report['subcluster_hit_pidents_min']) + '\t' +
                          str(report['subcluster_hit_pidents_max']) + '\n')
        report_file.write(
            ('  - intra-sample identity:').ljust(col) +
            str(report['subcluster_hit_pidents_intra_mean']) + '\t' +
            str(report['subcluster_hit_pidents_intra_sd']) + '\t' +
            str(report['subcluster_hit_pidents_intra_min']) + '\t' +
            str(report['subcluster_hit_pidents_intra_max']) + '\n')
        report_file.write(
            ('  - inter-sample identity:').ljust(col) +
            str(report['subcluster_hit_pidents_inter_mean']) + '\t' +
            str(report['subcluster_hit_pidents_inter_sd']) + '\t' +
            str(report['subcluster_hit_pidents_inter_min']) + '\t' +
            str(report['subcluster_hit_pidents_inter_max']) + '\n')
        report_file.write(('Subcluster mean hit ppositives:').ljust(col) +
                          str(report['subcluster_hit_pposs_mean']) + '\t' +
                          str(report['subcluster_hit_pposs_sd']) + '\t' +
                          str(report['subcluster_hit_pposs_min']) + '\t' +
                          str(report['subcluster_hit_pposs_max']) + '\n')
        report_file.write(
            ('  - intra-sample ppositives:').ljust(col) +
            str(report['subcluster_hit_pposs_intra_mean']) + '\t' +
            str(report['subcluster_hit_pposs_intra_sd']) + '\t' +
            str(report['subcluster_hit_pposs_intra_min']) + '\t' +
            str(report['subcluster_hit_pposs_intra_max']) + '\n')
        report_file.write(
            ('  - inter-sample ppositives:').ljust(col) +
            str(report['subcluster_hit_pposs_inter_mean']) + '\t' +
            str(report['subcluster_hit_pposs_inter_sd']) + '\t' +
            str(report['subcluster_hit_pposs_inter_min']) + '\t' +
            str(report['subcluster_hit_pposs_inter_max']) + '\n')
        report_file.write(('Subcluster mean prot length:').ljust(col) +
                          str(report['subcluster_prot_lengths_mean']) + '\t' +
                          str(report['subcluster_prot_lengths_sd']) + '\t' +
                          str(report['subcluster_prot_lengths_min']) + '\t' +
                          str(report['subcluster_prot_lengths_max']) + '\n')

    report_file.write('\n\nPARAMETERS\n')
    report_file.write('-'*80 + '\n\n')

    report_file.write(('Target protein file(s):').ljust(col) +
                      ', '.join(report['t']) + '\n')
    if not report['b']:
        if report['q'] == []:
            if not report['s']:
                report_file.write(('Operations:').ljust(col) +
                                  'mapping and clustering\n')
            else:
                report_file.write(('Operations:').ljust(col) +
                                  'mapping, clustering and subclustering\n')
        else:
            if not report['s']:
                report_file.write(('Operations:').ljust(col) + 'mapping\n')
            else:
                report_file.write(('Operations:').ljust(col) +
                                  'mapping and subclustering\n')
        report_file.write(('Sequence search method:').ljust(col))
        if report['q'] == []:
            report_file.write('De novo sequence search\n\n')
        else:
            report_file.write('Ab initio sequence search\n')
            report_file.write(('Query patterns file:').ljust(col) +
                              ' ,'.join(report['q']) + '\n\n')

    else:
        report_file.write(
            ('Operations:').ljust(col) +
            'direct subclustering (mapping and clustering by-passed)\n\n')

    report_file.write(('Configuration file:').ljust(col) + report['K'] + '\n')
    report_file.write(('Number of threads:').ljust(col) + str(report['T']) +
                      '\n\n')

    if not report['b']:

        report_file.write(('Patterning key residue:').ljust(col) +
                          report['k'] + '\n')
        report_file.write(('Minimum nb of key residues:').ljust(col) +
                          str(report['m']) + '\n')
        report_file.write(('Maximum nb of key residues:').ljust(col) +
                          str(report['M']) + '\n')
        report_file.write(('Minimum protein length:').ljust(col) +
                          str(report['l']) + '\n')
        report_file.write(('Maximum protein length:').ljust(col) +
                          str(report['L']) + '\n')
        report_file.write(('N-terminal tolerance:').ljust(col) +
                          str(report['n']) + '\n')
        report_file.write(('C-terminal tolerance:').ljust(col) +
                          str(report['c']) + '\n\n')

        report_file.write(('Stringency on identity:').ljust(col) +
                          str(10 * report['I']) + '\n')
        report_file.write(('Stringency on coverage:').ljust(col) +
                          str(10 * report['C']) + '\n')
        report_file.write(('Stringency on persistence:').ljust(col) +
                          str(10 * report['P']) + '\n\n')

        report_file.write(('Nb gained key residue allowed:').ljust(col) +
                          str(report['kg']) + '\n')
        report_file.write(('Nb lost key residue allowed:').ljust(col) +
                          str(report['kl']) + '\n')
        report_file.write(('Pattern inclusion allowed:').ljust(col) +
                          str(report['i']) + '\n\n')

        if report['q'] == []:
            report_file.write(('Score thr, clustering pass 1:').ljust(col) +
                              str(100 * report['S1']) + '\n')
            report_file.write(('Score thr, clustering pass 2:').ljust(col) +
                              str(100 * report['S2']) + '\n')
            report_file.write(('Fusion thr, clustering pass 2:').ljust(col) +
                              str(100 * report['F']) + '\n\n')

        else:
            report_file.write(('KAPPA-score threshold:').ljust(col) +
                              str(100 * report['S']) + '\n')

    if report['s']:

        report_file.write(('Minimum coverage:').ljust(col) +
                          str(100 * report['sC']) + '\n')
        report_file.write(('Minimum hit length:').ljust(col) +
                          str(report['sL']) + '\n')
        report_file.write(('Maximum e-value:').ljust(col) + str(report['sE']) +
                          '\n')
        if report['sR'] is None:
            report_file.write(('Reciprocity threshold:').ljust(col) +
                              str(report['sR']) + '\n')
        else:
            report_file.write(('Reciprocity threshold:').ljust(col) +
                              str(100 * report['sR']) + '\n')
        report_file.write(('Minimum identity on hit:').ljust(col) +
                          str(100 * report['sI']) + '\n')
        report_file.write(('Minimum ident. on shortest seq:').ljust(col) +
                          str(100 * report['sJ']) + '\n')
        report_file.write(('Minimum ppos. on hit:').ljust(col) +
                          str(100 * report['sP']) + '\n')
        report_file.write(('Minimum ppos. on shortest seq:').ljust(col) +
                          str(100 * report['sQ']) + '\n')

    report_file.write('\n')
    report_file.close()


def write_optimization_report(reports, file_id):

    optimization_report = open('Optimization_results_' + file_id + '.txt', 'w')

    optimization_report.write('KAPPA Optimization results\n')
    optimization_report.write('\n')
    optimization_report.write('Target proteins file(s): ' +
                              ', '.join(reports[1]['t']))
    optimization_report.write('\n')
    optimization_report.write('\n')

    if not reports[1]['b']:
        if reports[1]['q'] == []:
            if not reports[1]['s']:
                optimization_report.write(
                    'Operations: mapping and clustering\n')
            else:
                optimization_report.write(
                    'Operations: mapping, clustering and subclustering\n')
        else:
            if not reports[1]['s']:
                optimization_report.write('Operations: mapping\n')
            else:
                optimization_report.write(
                    'Operations: mapping and subclustering\n')
        if reports[1]['q'] == []:
            optimization_report.write(
                'Sequence search method: De novo sequence search\n')
        else:
            optimization_report.write(
                'Sequence search method: Ab initio sequence search\n')
            optimization_report.write('Query patterns file(s): ' +
                                      ', '.join(report['q']))

    else:
        optimization_report.write(
            'Operations: direct subclustering (mapping and clustering '
            'by-passed)\n\n')

    configuration_files = [reports[n]['K'] for n in reports.keys()]
    configuration_files = list(set(configuration_files))
    configuration_files.sort()

    optimization_report.write('\nCombinations' + '\t' + '\t'.join(
        [str(n) for n in sorted(reports.keys())]) + '\n\n')
    optimization_report.write('Configuration file*' + '\t' + '\t'.join([
        'File_' + str(1 + configuration_files.index(reports[n]['K']))
        for n in sorted(reports.keys())
    ]) + '\n')
    optimization_report.write('Number of threads' + '\t' + '\t'.join(
        [str(reports[n]['T']) for n in sorted(reports.keys())]) + '\n')

    if not reports[1]['b']:
        optimization_report.write('Patterning key residue' + '\t' + '\t'.join(
            [reports[n]['k'] for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Minimum nb of key residues' + '\t' +
            '\t'.join([str(reports[n]['m'])
                       for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Maximum nb of key residues' + '\t' +
            '\t'.join([str(reports[n]['M'])
                       for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Minimum protein length' + '\t' + '\t'.join(
            [str(reports[n]['l']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Maximum protein length' + '\t' + '\t'.join(
            [str(reports[n]['L']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('N-terminal tolerance' + '\t' + '\t'.join(
            [str(reports[n]['n']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('C-terminal tolerance' + '\t' + '\t'.join(
            [str(reports[n]['c']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Stringency on identity' + '\t' + '\t'.join(
            [str(10 * reports[n]['I'])
             for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Stringency on coverage' + '\t' + '\t'.join(
            [str(10 * reports[n]['C'])
             for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Stringency on persistence' + '\t' + '\t'.join(
                [str(10 * reports[n]['P'])
                 for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Gain of key residue allowed' + '\t' +
            '\t'.join([str(reports[n]['kg'])
                       for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Loss of key residue allowed' + '\t' +
            '\t'.join([str(reports[n]['kl'])
                       for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Pattern inclusion allowed' + '\t' +
            '\t'.join([str(reports[n]['i'])
                       for n in sorted(reports.keys())]) + '\n')
        if reports[1]['q'] == []:
            optimization_report.write('Score thr, clustering pass 1' + '\t' +
                                      '\t'.join([
                                          str(100 * reports[n]['S1'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
            optimization_report.write('Score thr, clustering pass 2' + '\t' +
                                      '\t'.join([
                                          str(100 * reports[n]['S2'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
            optimization_report.write('Fusion thr, clustering pass 2' + '\t' +
                                      '\t'.join([
                                          str(100 * reports[n]['F'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
        else:
            optimization_report.write('KAPPA-score threshold' + '\t' +
                                      '\t'.join([
                                          str(100 * reports[n]['S'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
    if reports[1]['s']:
        optimization_report.write('Minimum coverage' + '\t' + '\t'.join(
            [str(100 * reports[n]['sC'])
             for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Minimum hit length' + '\t' + '\t'.join(
            [str(reports[n]['sL']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Maximum e-value' + '\t' + '\t'.join(
            [str(reports[n]['sE']) for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Reciprocity threshold' + '\t' + '\t'.join([
            str(reports[n]['sR'])
            if reports[n]['sR'] is None else str(100 * reports[n]['sR'])
            for n in sorted(reports.keys())
        ]) + '\n')
        optimization_report.write('Minimum identity on hit' + '\t' + '\t'.join(
            [str(100 * reports[n]['sI'])
             for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Minimum ident. on shortest seq' + '\t' + '\t'.join(
                [str(100 * reports[n]['sJ'])
                 for n in sorted(reports.keys())]) + '\n')
        optimization_report.write('Minimum ppos. on hit' + '\t' + '\t'.join(
            [str(100 * reports[n]['sP'])
             for n in sorted(reports.keys())]) + '\n')
        optimization_report.write(
            'Minimum ppos. on shortest seq' + '\t' + '\t'.join(
                [str(100 * reports[n]['sQ'])
                 for n in sorted(reports.keys())]) + '\n')

    optimization_report.write('\nExecution time' + '\t' + '\t'.join(
        [str(reports[n]['execution_time'])
         for n in sorted(reports.keys())]) + '\n')

    if not reports[1]['b']:
        optimization_report.write(
            'Number of imported proteins' + '\t' + '\t'.join([
                str(reports[n]['imported_proteins_clustering'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write('Number of rejected proteins' + '\t' +
                                  '\t'.join([
                                      str(reports[n]['rejected_proteins'])
                                      for n in sorted(reports.keys())
                                  ]) + '\n')
        optimization_report.write('Number of retained proteins' + '\t' +
                                  '\t'.join([
                                      str(reports[n]['retained_proteins'])
                                      for n in sorted(reports.keys())
                                  ]) + '\n')
        optimization_report.write('Number of query patterns' + '\t' +
                                  '\t'.join([
                                      str(reports[n]['query_patterns'])
                                      for n in sorted(reports.keys())
                                  ]) + '\n')

        if reports[1]['q'] == []:

            optimization_report.write('Number of clusters built' + '\t' +
                                      '\t'.join([
                                          str(reports[n]['clusters'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
            optimization_report.write(
                'Number of singleton clusters' + '\t' + '\t'.join([
                    str(reports[n]['clustering_singletons'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per cluster Mean' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_cluster_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per cluster SD' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_cluster_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per cluster Min' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_cluster_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per cluster Max' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_cluster_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of edges per cluster Mean' + '\t' + '\t'.join([
                    str(reports[n]['edges_per_cluster_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of edges per cluster SD' + '\t' + '\t'.join([
                    str(reports[n]['edges_per_cluster_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of edges per cluster Min' + '\t' + '\t'.join([
                    str(reports[n]['edges_per_cluster_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of edges per cluster Max' + '\t' + '\t'.join([
                    str(reports[n]['edges_per_cluster_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per cluster Mean' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_cluster_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per cluster SD' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_cluster_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per cluster Min' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_cluster_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per cluster Max' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_cluster_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean protein length Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_prot_lengths_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean protein length SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_prot_lengths_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean protein length Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_prot_lengths_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean protein length Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_prot_lengths_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster_density Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write('Cluster_density SD' + '\t' + '\t'.join([
                str(reports[n]['cluster_densities_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
            optimization_report.write(
                'Cluster_density Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster_density Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample  density Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_intra_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample cl density SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_intra_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample cl density Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_intra_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample cl density Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_intra_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - inter-sample cl density Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_inter_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - inter-sample cl density SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_inter_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - inter-sample cl density Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_inter_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - inter-sample cl density Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_densities_inter_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean KAPPA-score Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean KAPPA-score SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean KAPPA-score Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean KAPPA-score Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_intra_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_intra_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_intra_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_intra_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_inter_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_inter_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_inter_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                '  - intra-sample k-score Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_kappa_scores_inter_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean identity (I) Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_identities_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean identity (I) SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_identities_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean identity (I) Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_identities_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean identity (I) Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_identities_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean coverage (C) Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_coverages_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean coverage (C) SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_coverages_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean coverage (C) Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_coverages_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean coverage (C) Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_coverages_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean persistence (P) Mean' + '\t' + '\t'.join([
                    str(reports[n]['cluster_persistences_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean persistence (P) SD' + '\t' + '\t'.join([
                    str(reports[n]['cluster_persistences_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean persistence (P) Min' + '\t' + '\t'.join([
                    str(reports[n]['cluster_persistences_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Cluster mean persistence (P) Max' + '\t' + '\t'.join([
                    str(reports[n]['cluster_persistences_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
        else:
            optimization_report.write('Number of groups formed' + '\t' +
                                      '\t'.join([
                                          str(reports[n]['groups'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
            optimization_report.write(
                'Query patterns with no hit' + '\t' + '\t'.join(
                    [str(reports[n]['nohit'])
                     for n in sorted(reports.keys())]) + '\n')
            optimization_report.write(
                'Nb of proteins per group Mean' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_group_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per group SD' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_group_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per group Min' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_group_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of proteins per group Max' + '\t' + '\t'.join([
                    str(reports[n]['proteins_per_group_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per group Mean' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_group_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per group SD' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_group_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per group Min' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_group_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Nb of samples per group Max' + '\t' + '\t'.join([
                    str(reports[n]['samples_per_group_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean protein length Mean' + '\t' + '\t'.join([
                    str(reports[n]['group_prot_lengths_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean protein length SD' + '\t' + '\t'.join([
                    str(reports[n]['group_prot_lengths_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean protein length Min' + '\t' + '\t'.join([
                    str(reports[n]['group_prot_lengths_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean protein length Max' + '\t' + '\t'.join([
                    str(reports[n]['group_prot_lengths_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean KAPPA-score Mean' + '\t' + '\t'.join([
                    str(reports[n]['group_kappa_scores_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean KAPPA-score SD' + '\t' + '\t'.join([
                    str(reports[n]['group_kappa_scores_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean KAPPA-score Min' + '\t' + '\t'.join([
                    str(reports[n]['group_kappa_scores_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean KAPPA-score Max' + '\t' + '\t'.join([
                    str(reports[n]['group_kappa_scores_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean identity (I) Mean' + '\t' + '\t'.join([
                    str(reports[n]['group_identities_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean identity (I) SD' + '\t' + '\t'.join([
                    str(reports[n]['group_identities_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean identity (I) Min' + '\t' + '\t'.join([
                    str(reports[n]['group_identities_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean identity (I) Max' + '\t' + '\t'.join([
                    str(reports[n]['group_identities_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean coverage (C) Mean' + '\t' + '\t'.join([
                    str(reports[n]['group_coverages_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write('Group mean coverage (C) SD' + '\t' +
                                      '\t'.join([
                                          str(reports[n]['group_coverages_sd'])
                                          for n in sorted(reports.keys())
                                      ]) + '\n')
            optimization_report.write(
                'Group mean coverage (C) Min' + '\t' + '\t'.join([
                    str(reports[n]['group_coverages_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean coverage (C) Max' + '\t' + '\t'.join([
                    str(reports[n]['group_coverages_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean persistence (P) Mean' + '\t' + '\t'.join([
                    str(reports[n]['group_persistences_mean'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean persistence (P) SD' + '\t' + '\t'.join([
                    str(reports[n]['group_persistences_sd'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean persistence (P) Min' + '\t' + '\t'.join([
                    str(reports[n]['group_persistences_min'])
                    for n in sorted(reports.keys())
                ]) + '\n')
            optimization_report.write(
                'Group mean persistence (P) Max' + '\t' + '\t'.join([
                    str(reports[n]['group_persistences_max'])
                    for n in sorted(reports.keys())
                ]) + '\n')

    if reports[1]['s']:

        optimization_report.write('\nNumber of imported clusters' + '\t' +
                                  '\t'.join([
                                      str(reports[n]['imported_clusters'])
                                      for n in sorted(reports.keys())
                                  ]) + '\n')
        optimization_report.write(
            'Number of imported proteins' + '\t' + '\t'.join([
                str(reports[n]['imported_proteins_subclustering'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write('Number of subclusters built' + '\t' +
                                  '\t'.join([
                                      str(reports[n]['nb_subclusters'])
                                      for n in sorted(reports.keys())
                                  ]) + '\n')
        optimization_report.write(
            'Number of singleton subclusters' + '\t' + '\t'.join([
                str(reports[n]['nb_singletons_subclusters'])
                for n in sorted(reports.keys())
            ]) + '\n')

        optimization_report.write(
            'Nb of proteins per subcluster Mean' + '\t' + '\t'.join([
                str(reports[n]['proteins_per_subcluster_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of proteins per subcluster SD' + '\t' + '\t'.join([
                str(reports[n]['proteins_per_subcluster_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of proteins per subcluster Min' + '\t' + '\t'.join([
                str(reports[n]['proteins_per_subcluster_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of proteins per subcluster Max' + '\t' + '\t'.join([
                str(reports[n]['proteins_per_subcluster_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of edges per subcluster Mean' + '\t' + '\t'.join([
                str(reports[n]['edges_per_subcluster_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of edges per subcluster SD' + '\t' + '\t'.join([
                str(reports[n]['edges_per_subcluster_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of edges per subcluster Min' + '\t' + '\t'.join([
                str(reports[n]['edges_per_subcluster_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of edges per subcluster Max' + '\t' + '\t'.join([
                str(reports[n]['edges_per_subcluster_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of samples per subcluster Mean' + '\t' + '\t'.join([
                str(reports[n]['samples_per_subcluster_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of samples per subcluster SD' + '\t' + '\t'.join([
                str(reports[n]['samples_per_subcluster_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of samples per subcluster Min' + '\t' + '\t'.join([
                str(reports[n]['samples_per_subcluster_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Nb of samples per subcluster Max' + '\t' + '\t'.join([
                str(reports[n]['samples_per_subcluster_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster density Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write('Subcluster density SD' + '\t' + '\t'.join([
            str(reports[n]['subcluster_densities_sd'])
            for n in sorted(reports.keys())
        ]) + '\n')
        optimization_report.write('Subcluster density Min' + '\t' + '\t'.join([
            str(reports[n]['subcluster_densities_min'])
            for n in sorted(reports.keys())
        ]) + '\n')
        optimization_report.write('Subcluster density Max' + '\t' + '\t'.join([
            str(reports[n]['subcluster_densities_max'])
            for n in sorted(reports.keys())
        ]) + '\n')
        optimization_report.write(
            '  - intra-sample sub density Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_intra_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample sub density SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_intra_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample sub density Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_intra_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample sub density Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_intra_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample sub density Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_inter_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample sub density SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_inter_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample sub density Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_inter_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample sub density Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_densities_inter_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit identity Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit identity SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit identity Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit identity Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample identity Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_intra_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample identity SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_intra_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample identity Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_intra_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample identity Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_intra_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample identity Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_inter_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample identity SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_inter_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample identity Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_inter_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample identity Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pidents_inter_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit ppositives Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit ppositives SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit ppositives Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean hit ppositives Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample ppositives Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_intra_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample ppositives SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_intra_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample ppositives Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_intra_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - intra-sample ppositives Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_intra_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample ppositives Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_inter_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample ppositives SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_inter_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample ppositives Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_inter_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            '  - inter-sample ppositives Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_hit_pposs_inter_max'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean prot length Mean' + '\t' + '\t'.join([
                str(reports[n]['subcluster_prot_lengths_mean'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean prot length SD' + '\t' + '\t'.join([
                str(reports[n]['subcluster_prot_lengths_sd'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean prot length Min' + '\t' + '\t'.join([
                str(reports[n]['subcluster_prot_lengths_min'])
                for n in sorted(reports.keys())
            ]) + '\n')
        optimization_report.write(
            'Subcluster mean prot length Max' + '\t' + '\t'.join([
                str(reports[n]['subcluster_prot_lengths_max'])
                for n in sorted(reports.keys())
            ]) + '\n')

    optimization_report.write(
        '\n\n*Configuration files short names in the table above:\n')
    for i in range(len(configuration_files)):
        optimization_report.write('File_' + str(i + 1) + ': ' +
                                  configuration_files[i] + '\n')

    optimization_report.close()


def remove_outputs(file_id):

    output_directories = [
        'Clusters_' + file_id, 'Subclusters_' + file_id, 'Groups_' + file_id
    ]
    output_files = [
        'Mapping_scores_' + file_id + '.txt',
        'Clusters_Table_' + file_id + '.txt',
        'Clusters_Stats_' + file_id + '.txt',
        'Groups_Table_' + file_id + '.txt', 'Groups_Stats_' + file_id + '.txt',
        'Cytoscape_Clusters_' + file_id + '.txt',
        'Cytoscape_Clusters_attributes_' + file_id + '.txt',
        'Subclusters_Table_' + file_id + '.txt',
        'Subclusters_Stats_' + file_id + '.txt',
        'Cytoscape_Subclusters_' + file_id + '.txt',
        'Cytoscape_Subclusters_attributes_' + file_id + '.txt',
        'Subclustering_' + file_id + '_makeblastdb.log',
        'Subclustering_' + file_id + '_blastp.log'
    ]

    for output_directory in output_directories:
        if os.path.exists(output_directory):
            shutil.rmtree(output_directory)

    for output_file in output_files:
        if os.path.exists(output_file):
            os.remove(output_file)


if __name__ == '__main__':

    print('')
    print('*'*80)
    print('*' + ' '*15 + 'KEY AMINOACID PATTERN-BASED PROTEIN ANALYZER 1.1' +
          ' '*15 + '*')
    print('*'*80)
    print('')

    # Arguments definition

    parser = argparse.ArgumentParser(
        description='Key Aminoacids Pattern-based Protein Analyzer.',
        add_help=False)

    # General parameters
    group1 = parser.add_argument_group('General parameters')
    group1.add_argument(
        '-t',
        '--target',
        metavar='FILE',
        nargs='+',
        required=True,
        help='Target proteins.')
    group1.add_argument(
        '-q',
        '--query',
        metavar='FILE',
        nargs='+',
        default=None,
        help='Query patterns.')
    group1.add_argument(
        '-b',
        '--by_pass',
        default=False,
        action='store_true',
        help='By-pass mapping and clustering (do subclustering only).')
    group1.add_argument(
        '-K',
        '--kappa_config',
        metavar='FILE',
        nargs='+',
        default=[
            os.path.join(
                os.path.dirname(os.path.realpath(__file__)), 'kappa.config')
        ],
        help='KAPPA configuration file.')
    group1.add_argument(
        '-o',
        '--output',
        metavar='STR',
        default=None,
        help='Identifier for output files.')
    group1.add_argument(
        '-r',
        '--report',
        default=False,
        action='store_true',
        help='Make report file.')
    group1.add_argument(
        '-G',
        '--graphs',
        default=False,
        action='store_true',
        help='Generate Cytoscape inputs.')
    group1.add_argument(
        '-T',
        '--threads',
        metavar='INT',
        nargs='+',
        default=[1],
        help='Number of threads.')
    group1.add_argument(
        '-h', '--help', action='help', help='Print this help page and exit.')
    group1.add_argument(
        '-V',
        '--version',
        action='version',
        version='Current version: KAPPA 1.1',
        help='Show program\'s version number and exit.')

    # Sequence filtering parameters
    group2 = parser.add_argument_group('Sequence filtering parameters')
    group2.add_argument(
        '-k', '--key_residue', metavar='AA', default='C', help='Key residue.')
    group2.add_argument(
        '-m',
        '--minkey',
        metavar='INT',
        nargs='+',
        default=[4],
        help='Min number of keys.')
    group2.add_argument(
        '-M',
        '--maxkey',
        metavar='INT',
        nargs='+',
        default=[None],
        help='Max number of keys.')
    group2.add_argument(
        '-l',
        '--minlength',
        metavar='INT',
        nargs='+',
        default=[0],
        help='Min protein length.')
    group2.add_argument(
        '-L',
        '--maxlength',
        metavar='INT',
        nargs='+',
        default=[None],
        help='Max protein length.')

    # Mapping and clustering parameters
    group3 = parser.add_argument_group('Mapping and clustering parameters')
    group3.add_argument(
        '-n',
        '--nterm',
        metavar='INT',
        nargs='+',
        default=[1],
        help='Ignored N-term blocks.')
    group3.add_argument(
        '-c',
        '--cterm',
        metavar='INT',
        nargs='+',
        default=[1],
        help='Ignored C-term blocks.')
    group3.add_argument(
        '-kg',
        '--key_gain',
        metavar='INT',
        nargs='+',
        default=[0],
        help='Allowed gained key residues.')
    group3.add_argument(
        '-kl',
        '--key_loss',
        metavar='INT',
        nargs='+',
        default=[0],
        help='Allowed lost key residues.')
    group3.add_argument(
        '-i',
        '--allow_inclusion',
        metavar='True, False',
        nargs='*',
        default=[False],
        help='Allow pattern inclusion.')

    group3.add_argument(
        '-I',
        '--identity',
        metavar='FLOAT',
        nargs='+',
        default=[7.0],
        help='Stringency on block identity.')
    group3.add_argument(
        '-C',
        '--coverage',
        metavar='FLOAT',
        nargs='+',
        default=[7.0],
        help='Stringency on alignment coverage.')
    group3.add_argument(
        '-P',
        '--persistence',
        metavar='FLOAT',
        nargs='+',
        default=[7.0],
        help='Stringency on pattern persistence.')

    group3.add_argument(
        '-S',
        '--score',
        metavar='FLOAT',
        nargs='+',
        default=[90.0],
        help='Ab initio mapping: KAPPA score threshold.')
    group3.add_argument(
        '-S1',
        '--score1',
        metavar='FLOAT',
        nargs='+',
        default=[90.0],
        help='De novo clustering: Score threshold 1.')
    group3.add_argument(
        '-S2',
        '--score2',
        metavar='FLOAT',
        nargs='+',
        default=[80.0],
        help='De novo clustering: Score threshold 2.')
    group3.add_argument(
        '-F',
        '--fusion_thr',
        metavar='FLOAT',
        nargs='+',
        default=[70.0],
        help='De novo clustering: Fusion threshold.')
    group3.add_argument(
        '-e',
        '--export_scores',
        default=False,
        action='store_true',
        help='Export all mapping scores.')

    # Subclustering parameters

    group4 = parser.add_argument_group('Subclustering parameters')
    group4.add_argument(
        '-s',
        '--subclustering',
        default=False,
        action='store_true',
        help='Perform subclustering.')
    group4.add_argument(
        '-sC',
        '--min_coverage',
        metavar='FLOAT',
        nargs='+',
        default=[90.0],
        help='Subclustering: min coverage.')
    group4.add_argument(
        '-sL',
        '--min_hit_length',
        metavar='FLOAT',
        nargs='+',
        default=[50],
        help='Subclustering: min hit length.')

    group4.add_argument(
        '-sE',
        '--max_evalue',
        metavar='FLOAT',
        nargs='+',
        default=[1e-05],
        help='Subclustering: max e-value.')
    group4.add_argument(
        '-sR',
        '--reciprocity',
        metavar='FLOAT',
        nargs='+',
        default=[None],
        help='Subclustering: reciprocity tolerance.')

    group4.add_argument(
        '-sI',
        '--min_hit_pident',
        metavar='FLOAT',
        nargs='+',
        default=[20.0],
        help='Subclustering: min identity on hit.')
    group4.add_argument(
        '-sJ',
        '--min_shortest_seq_pident',
        metavar='FLOAT',
        nargs='+',
        default=[20.0],
        help='Subclustering: min identity on shortest seq.')
    group4.add_argument(
        '-sP',
        '--min_hit_ppos',
        metavar='FLOAT',
        nargs='+',
        default=[20.0],
        help='Subclustering: min identity on shortest seq.')
    group4.add_argument(
        '-sQ',
        '--min_shortest_seq_ppos',
        metavar='FLOAT',
        nargs='+',
        default=[20.0],
        help='Subclustering: min identity on shortest seq.')

    # Arguments assignment
    args = parser.parse_args()

    target_paths = args.target
    query_paths = args.query
    de_novo = False
    if query_paths is None:
        de_novo = True
    by_pass = args.by_pass
    file_id = args.output
    list_config_file_name = args.kappa_config
    make_report = args.report
    list_threads = args.threads

    key_residue = args.key_residue
    list_minkey = args.minkey
    list_maxkey = args.maxkey
    list_minlength = args.minlength
    list_maxlength = args.maxlength

    list_nterm = args.nterm
    list_cterm = args.cterm
    list_gain = args.key_gain
    list_loss = args.key_loss
    list_allow_inclusion = args.allow_inclusion
    list_identity_str = args.identity
    list_coverage_str = args.coverage
    list_persistence_str = args.persistence
    list_score_thr = args.score
    list_score_thr_pass1 = args.score1
    list_score_thr_pass2 = args.score2
    list_fusion_thr = args.fusion_thr
    export_scores = args.export_scores

    subclustering = args.subclustering
    list_max_evalue = args.max_evalue
    list_min_hit_pident = args.min_hit_pident
    list_min_shortest_seq_pident = args.min_shortest_seq_pident
    list_min_hit_ppos = args.min_hit_ppos
    list_min_shortest_seq_ppos = args.min_shortest_seq_ppos
    list_min_hit_length = args.min_hit_length
    list_reciprocity = args.reciprocity
    list_min_coverage = args.min_coverage
    graphs = args.graphs

    # Check arguments

    errors = []

    try:
        if by_pass and (
                query_paths is not None or key_residue != 'C'
                or list_minkey != [4] or list_maxkey != [None]
                or list_minlength != [0] or list_maxlength != [None]
                or list_nterm != [1] or list_cterm != [1] or list_gain != [0]
                or list_loss != [0] or list_allow_inclusion != [False]
                or list_score_thr != [90.0] or list_score_thr_pass1 != [90.0]
                or list_score_thr_pass2 != [80.0] or list_fusion_thr != [70.0]
                or list_identity_str != [7.0] or list_coverage_str != [7.0]
                or list_persistence_str != [7.0] or export_scores):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: Option -b is not compatible with options -c, -C, -e, -F, '
            '-g, -i, -I, -k, -l, -L, -m, -M, -n, -P, -q, -S, -S1, -S2 and -t.')

    try:
        if not subclustering and (by_pass or list_max_evalue != [1e-05]
                                  or list_min_hit_pident != [20.0]
                                  or list_min_shortest_seq_pident != [20.0]
                                  or list_min_hit_ppos != [20.0]
                                  or list_min_shortest_seq_ppos != [20.0]
                                  or list_min_hit_length != [50]
                                  or list_reciprocity != [None]
                                  or list_min_coverage != [90]):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: Options -b, -sC, -sE, -sI, -sJ, -sL, -sP, -sQ and -sR '
            'require option -s.')

    try:
        if list_score_thr != [90.0] and de_novo:
            raise ValueError
    except ValueError:
        errors.append('ERROR: Option -S requires option -q.')

    try:
        if not de_novo and (list_score_thr_pass1 != [90.0]
                            or list_score_thr_pass2 != [80.0]
                            or list_fusion_thr != [70.0]):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: Options -S1, -S2 and -F are not compatible with option -q.'
        )

    try:
        if list_score_thr != [90.0] and (list_score_thr_pass1 != [90.0]
                                         or list_score_thr_pass2 != [80.0]
                                         or list_fusion_thr != [70.0]):
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: Option -S is not compatible with options -S1, -S2 and -F.')

    target_proteins_files = []
    for target_path in target_paths:
        try:
            if not os.path.exists(target_path):
                raise ValueError
            elif os.path.isfile(target_path) and isfasta(target_path):
                target_proteins_files.append(target_path)
            elif os.path.isdir(target_path):
                for target_file_name in os.listdir(target_path):
                    target_file_path = os.path.join(target_path,
                                                    target_file_name)
                    if os.path.isfile(target_file_path) and isfasta(
                            target_file_path):
                        target_proteins_files.append(target_file_path)
        except ValueError:
            errors.append('ERROR: -t: File ' + target_path +
                          ' does not exist.')

    try:
        if len(target_proteins_files) == 0:
            raise ValueError
    except ValueError:
        errors.append('ERROR: -t: No input FASTA file detected.')

    query_patterns_files = []
    if query_paths is not None:
        for query_path in query_paths:
            try:
                if not os.path.exists(query_path):
                    raise ValueError
                elif os.path.isfile(query_path):
                    query_patterns_files.append(query_path)
                elif os.path.isdir(query_path):
                    for query_file_name in os.listdir(query_path):
                        query_file_path = os.path.join(query_path,
                                                       query_file_name)
                        if os.path.isfile(query_file_path):
                            query_patterns_files.append(query_file_path)
            except ValueError:
                errors.append('ERROR: -q: File ' + query_path +
                              ' does not exist.')

    if file_id is not None:
        try:
            for item in [
                    'Clusters_' + file_id, 'Subclusters_' + file_id,
                    'Mapping_scores_' + file_id + '.txt',
                    'Clusters_Table_' + file_id + '.txt',
                    'Clusters_Stats_' + file_id + '.txt',
                    'Cytoscape_Clusters_' + file_id + '.txt',
                    'Cytoscape_Clusters_attributes_' + file_id + '.txt',
                    'Subclusters_Table_' + file_id + '.txt',
                    'Subclusters_Stats_' + file_id + '.txt',
                    'Cytoscape_Subclusters_' + file_id + '.txt',
                    'Cytoscape_Subclusters_attributes_' + file_id + '.txt',
                    'Report_' + file_id + '.txt',
                    'Optimization_results_' + file_id + '.txt',
                    'Subclustering_' + file_id + '_makeblastdb.log',
                    'Subclustering_' + file_id + '_blastp.log'
            ]:
                if item in os.listdir('.'):
                    raise ValueError
                    break
        except ValueError:
            errors.append(
                'ERROR: -o: This output identifier is already used in present '
                'working directory.')
    else:
        file_id = datetime.datetime.now().strftime('%y-%m-%d_%H-%M-%S')

    for config_file_name in list_config_file_name:
        try:
            if not os.path.exists(config_file_name):
                raise ValueError
        except ValueError:
            errors.append('ERROR: -K: File ' + config_file_name +
                          ' does not exist.')
    list_config_file_name = list(set(list_config_file_name))
    list_config_file_name.sort()

    final_threads = []
    for threads in list_threads:
        try:
            if int(threads) < 1 or int(threads) > multiprocessing.cpu_count():
                raise ValueError
                break
            else:
                final_threads.append(int(threads))
        except ValueError:
            errors.append(
                'ERROR: -T: Number of threads must be an integer between 1 and '
                + str(multiprocessing.cpu_count()) + '.')
    list_threads = list(set(final_threads))
    list_threads.sort()

    try:
        if len(key_residue) != 1 and key_residue not in 'ACDEFGHIKLMNPQRSTVWY':
            raise ValueError
    except ValueError:
        errors.append(
            'ERROR: -k: Key residue must be one letter belonging to '
            'ACDEFGHIKLMNPQRSTVWY.')

    final_minkey = []
    for minkey in list_minkey:
        try:
            if int(minkey) < 1:
                raise ValueError
                break
            else:
                final_minkey.append(int(minkey))
        except ValueError:
            errors.append(
                'ERROR: -m: Minimum key number must be an integer >= 2.')
    list_minkey = list(set(final_minkey))
    list_minkey.sort()

    final_maxkey = []
    no_max = False
    for maxkey in list_maxkey:
        if maxkey is not None and maxkey != 'None':
            try:
                if int(maxkey) < 0:
                    raise ValueError
                    break
                else:
                    final_maxkey.append(int(maxkey))
            except ValueError:
                errors.append(
                    'ERROR: -M: Maximum key number must be an integer >= 0.')
        else:
            no_max = True
    list_maxkey = list(set(final_maxkey))
    list_maxkey.sort()
    if no_max:
        list_maxkey.append(None)

    final_minlength = []
    for minlength in list_minlength:
        try:
            if int(minlength) < 0:
                raise ValueError
                break
            else:
                final_minlength.append(int(minlength))
        except ValueError:
            errors.append(
                'ERROR: -l: Minimum protein length must be an integer >= 0')
    list_minlength = list(set(final_minlength))
    list_minlength.sort()

    final_maxlength = []
    no_max = False
    for maxlength in list_maxlength:
        if maxlength is not None and maxlength != 'None':
            try:
                if int(maxlength) < 0:
                    raise ValueError
                    break
                else:
                    final_maxlength.append(int(maxlength))
            except ValueError:
                errors.append(
                    'ERROR: -L: Maximum protein length must be an integer '
                    '>= 0.')
        else:
            no_max = True
    list_maxlength = list(set(final_maxlength))
    list_maxlength.sort()
    if no_max:
        list_maxlength.append(None)

    final_nterm = []
    for nterm in list_nterm:
        try:
            if int(nterm) < 0:
                raise ValueError
                break
            else:
                final_nterm.append(int(nterm))
        except ValueError:
            errors.append(
                'ERROR: -n: N-term tolerance must be an integer >= 0.')
    list_nterm = list(set(final_nterm))
    list_nterm.sort()

    final_cterm = []
    for cterm in list_cterm:
        try:
            if int(cterm) < 0:
                raise ValueError
                break
            else:
                final_cterm.append(int(cterm))
        except ValueError:
            errors.append(
                'ERROR: -c: C-term tolerance must be an integer >= 0.')
    list_cterm = list(set(final_cterm))
    list_cterm.sort()

    final_gain = []
    for gain in list_gain:
        try:
            if int(gain) < 0:
                raise ValueError
                break
            else:
                final_gain.append(int(gain))
        except ValueError:
            errors.append(
                'ERROR: -kg: Number of allowed gained key aa must be an '
                'integer >= 0')
    list_gain = list(set(final_gain))
    list_gain.sort()

    final_loss = []
    for loss in list_loss:
        try:
            if int(loss) < 0:
                raise ValueError
                break
            else:
                final_loss.append(int(loss))
        except ValueError:
            errors.append(
                'ERROR: -kl: Number of allowed lost key aa must be an '
                'integer >= 0')
    list_loss = list(set(final_loss))
    list_loss.sort()

    final_allow_inclusion = []
    try:
        if list_allow_inclusion == []:
            final_allow_inclusion.append(True)
        else:
            for allow_inclusion in list_allow_inclusion:
                if allow_inclusion is False:
                    final_allow_inclusion.append(False)
                elif allow_inclusion.lower() in ['false', 'no', 'off', 'n']:
                    final_allow_inclusion.append(False)
                elif allow_inclusion.lower() in ['true', 'yes', 'on', 'y']:
                    final_allow_inclusion.append(True)
                else:
                    raise ValueError
                    break
    except ValueError:
        errors.append('ERROR: -i: Possible values are "True" and "False".')
    list_allow_inclusion = list(set(final_allow_inclusion))
    list_allow_inclusion.sort()

    final_identity_str = []
    for identity_str in list_identity_str:
        try:
            if float(identity_str) < 0.0:
                raise ValueError
                break
            else:
                final_identity_str.append(float(identity_str) / 10.0)
        except ValueError:
            errors.append(
                'ERROR: -I: Stringency on identity must be a real number '
                '>= 0.0')
    list_identity_str = list(set(final_identity_str))
    list_identity_str.sort()

    final_coverage_str = []
    for coverage_str in list_coverage_str:
        try:
            if float(coverage_str) < 0.0:
                raise ValueError
                break
            else:
                final_coverage_str.append(float(coverage_str) / 10.0)
        except ValueError:
            errors.append(
                'ERROR: -C: Stringency on coverage must be a real number '
                '>= 0.0')
    list_coverage_str = list(set(final_coverage_str))
    list_coverage_str.sort()

    final_persistence_str = []
    for persistence_str in list_persistence_str:
        try:
            if float(persistence_str) < 0.0:
                raise ValueError
                break
            else:
                final_persistence_str.append(float(persistence_str) / 10.0)
        except ValueError:
            errors.append(
                'ERROR: -P: Stringency on persistence must be a real number '
                '>= 0.0')
    list_persistence_str = list(set(final_persistence_str))
    list_persistence_str.sort()

    final_score_thr = []
    for score_thr in list_score_thr:
        try:
            if float(score_thr) < 0.0 or float(score_thr) > 100.0:
                raise ValueError
                break
            else:
                final_score_thr.append(float(score_thr) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -S: Score threshold must be a real number between '
                '0.0 and 100.0')
    list_score_thr = list(set(final_score_thr))
    list_score_thr.sort()

    final_score_thr_pass1 = []
    for score_thr_pass1 in list_score_thr_pass1:
        try:
            if float(score_thr_pass1) < 0.0 or float(score_thr_pass1) > 100.0:
                raise ValueError
                break
            else:
                final_score_thr_pass1.append(float(score_thr_pass1) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -S1: Score threshold 1 must be a real number between '
                '0.0 and 100.0')
    list_score_thr_pass1 = list(set(final_score_thr_pass1))
    list_score_thr_pass1.sort()

    final_score_thr_pass2 = []
    for score_thr_pass2 in list_score_thr_pass2:
        try:
            if float(score_thr_pass2) < 0.0 or float(score_thr_pass2) > 100.0:
                raise ValueError
                break
            else:
                final_score_thr_pass2.append(float(score_thr_pass2) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -S2: Score threshold 2 must be a real number between '
                '0.0 and 100.0')
    list_score_thr_pass2 = list(set(final_score_thr_pass2))
    list_score_thr_pass2.sort()

    final_fusion_thr = []
    for fusion_thr in list_fusion_thr:
        try:
            if float(fusion_thr) < 0.0 or float(fusion_thr) > 100.0:
                raise ValueError
                break
            else:
                final_fusion_thr.append(float(fusion_thr) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -F: Fusion threshold must be a real number between '
                '0.0 and 100.0')
    list_fusion_thr = list(set(final_fusion_thr))
    list_fusion_thr.sort()

    final_max_evalue = []
    for max_evalue in list_max_evalue:
        try:
            if float(max_evalue) < 0.0:
                raise ValueError
                break
            else:
                final_max_evalue.append(float(max_evalue))
        except ValueError:
            errors.append(
                'ERROR: -sE: Maximum evalue must be a real number >= 0.0')
    list_max_evalue = list(set(final_max_evalue))
    list_max_evalue.sort()

    final_min_hit_pident = []
    for min_hit_pident in list_min_hit_pident:
        try:
            if float(min_hit_pident) < 0.0 or float(min_hit_pident) > 100.0:
                raise ValueError
                break
            else:
                final_min_hit_pident.append(float(min_hit_pident) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -sI: Minimum identity on hit must be a real number '
                'between 0.0 and 100.0')
    list_min_hit_pident = list(set(final_min_hit_pident))
    list_min_hit_pident.sort()

    final_min_shortest_seq_pident = []
    for min_shortest_seq_pident in list_min_shortest_seq_pident:
        try:
            if float(min_shortest_seq_pident) < 0.0 or float(
                    min_shortest_seq_pident) > 100.0:
                raise ValueError
                break
            else:
                final_min_shortest_seq_pident.append(
                    float(min_shortest_seq_pident) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -sJ: Minimum identity on shortest sequence must be '
                'a real number between 0.0 and 100.0')
    list_min_shortest_seq_pident = list(set(final_min_shortest_seq_pident))
    list_min_shortest_seq_pident.sort()

    final_min_hit_ppos = []
    for min_hit_ppos in list_min_hit_ppos:
        try:
            if float(min_hit_ppos) < 0.0 or float(min_hit_ppos) > 100.0:
                raise ValueError
                break
            else:
                final_min_hit_ppos.append(float(min_hit_ppos) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -sP: Minimum percentage of positives on hit must be '
                'a real number between 0.0 and 100.0')
    list_min_hit_ppos = list(set(final_min_hit_ppos))
    list_min_hit_ppos.sort()

    final_min_shortest_seq_ppos = []
    for min_shortest_seq_ppos in list_min_shortest_seq_ppos:
        try:
            if float(min_shortest_seq_ppos) < 0.0 or float(
                    min_shortest_seq_ppos) > 100.0:
                raise ValueError
                break
            else:
                final_min_shortest_seq_ppos.append(
                    float(min_shortest_seq_ppos) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -sQ: Minimum percentage of positives on shortest '
                'sequence must be a real number between 0.0 and 100.0')
    list_min_shortest_seq_ppos = list(set(final_min_shortest_seq_ppos))
    list_min_shortest_seq_ppos.sort()

    final_min_hit_length = []
    for min_hit_length in list_min_hit_length:
        try:
            if int(min_hit_length) < 0:
                raise ValueError
                break
            else:
                final_min_hit_length.append(int(min_hit_length))
        except ValueError:
            errors.append(
                'ERROR: -sL: Minimum hit length must be an integer >= 0')
    list_min_hit_length = list(set(final_min_hit_length))
    list_min_hit_length.sort()

    final_reciprocity = []
    no_thr = False
    for reciprocity in list_reciprocity:
        if reciprocity is not None and reciprocity != 'None':
            try:
                if float(reciprocity) < 0.0:
                    raise ValueError
                    break
                else:
                    final_reciprocity.append(float(reciprocity) / 100.0)
            except ValueError:
                errors.append(
                    'ERROR: -sR: Reciprocity threshold must be a real '
                    'number >= 0.0')
        else:
            no_thr = True
    list_reciprocity = list(set(final_reciprocity))
    list_reciprocity.sort()
    if no_thr:
        list_reciprocity.append(None)

    final_min_coverage = []
    for min_coverage in list_min_coverage:
        try:
            if float(min_coverage) < 0.0 or float(min_coverage) > 100.0:
                raise ValueError
                break
            else:
                final_min_coverage.append(float(min_coverage) / 100.0)
        except ValueError:
            errors.append(
                'ERROR: -sC: Minimum coverage must be a real number between '
                '0.0 and 100.0')
    list_min_coverage = list(set(final_min_coverage))
    list_min_coverage.sort()

    if subclustering:
        (blastp_path, makeblastdb_path, blastp_options,
            config_errors) = read_config_file(config_file_name)
        errors.extend(config_errors)

    if errors != []:
        for error in errors:
            sys.stderr.write(error + '\n')
        sys.stderr.write('\n')
        sys.exit(1)

    # Execution

    combinations = list(
        itertools.product(*[
            list_config_file_name, list_threads, list_minkey, list_maxkey,
            list_minlength, list_maxlength, list_nterm, list_cterm, list_gain,
            list_loss, list_allow_inclusion, list_identity_str,
            list_coverage_str, list_persistence_str, list_score_thr,
            list_score_thr_pass1, list_score_thr_pass2, list_fusion_thr,
            list_max_evalue, list_min_hit_pident, list_min_shortest_seq_pident,
            list_min_hit_ppos, list_min_shortest_seq_ppos, list_min_hit_length,
            list_reciprocity, list_min_coverage
        ]))

    if len(combinations) == 1:
        optimization = False
    else:
        optimization = True
        optimization_reports = {}
        valid = {'yes': True, 'y': True, 'ye': True, 'no': False, 'n': False}
        print('WARNING: Two or more values were entered for at '
              'least 1 parameter.')
        print('         You are entering KAPPA optimization mode.')
        print('         ' + str(len(combinations)) +
              ' combinations of parameters will be tested')
        print('')
        print('Do you want to continue? (Y/n) ', end='')
        while True:
            choice = input().lower()
            if choice == '':
                go_on = True
                break
            elif choice in valid:
                go_on = valid[choice]
                break
            else:
                print('Please answer with "yes" or "no": ', end='')
        print('')
        if not go_on:
            sys.exit(0)

    n = 0

    for combination in combinations:

        n += 1
        print_stdout(
            'Testing combination ' + str(n) + ' out of ' + str(
                len(combinations)) + '...',
            level=0,
            end='',
            condition=optimization)

        config_file_name = combination[0]
        threads = combination[1]
        minkey = combination[2]
        maxkey = combination[3]
        minlength = combination[4]
        maxlength = combination[5]
        nterm = combination[6]
        cterm = combination[7]
        gain = combination[8]
        loss = combination[9]
        allow_inclusion = combination[10]
        identity_str = combination[11]
        coverage_str = combination[12]
        persistence_str = combination[13]
        score_thr = combination[14]
        score_thr_pass1 = combination[15]
        score_thr_pass2 = combination[16]
        fusion_thr = combination[17]
        max_evalue = combination[18]
        min_hit_pident = combination[19]
        min_shortest_seq_pident = combination[20]
        min_hit_ppos = combination[21]
        min_shortest_seq_ppos = combination[22]
        min_hit_length = combination[23]
        reciprocity = combination[24]
        min_coverage = combination[25]

        errors = []

        if maxkey is not None and maxkey < minkey:
            if optimization:
                errors.append(
                    'WARNING: Combination ignored (incompatibility: -M < -m).')
            else:
                errors.append(
                    'ERROR: -m/-M: Maximal number of key residues cannot '
                    'be smaller than the minimum.')

        if maxlength is not None and maxlength < minlength:
            if optimization:
                errors.append(
                    'WARNING: Combination ignored (incompatibility: -L < -l).')
            else:
                errors.append(
                    'ERROR: -l/-L: Maximal protein length cannot '
                    'be smaller than the minimum.')

        if nterm + cterm > minkey:
            if optimization:
                errors.append(
                    'WARNING: Combination ignored (incompatibility: '
                    '-n + -c > -m).')
            else:
                errors.append(
                    'ERROR: -m/-n/-c: Minimum number of key residues must '
                    'be >= nterm+cterm.')

        if errors != []:
            if optimization:
                for error in errors:
                    print_stderr(
                        error, level=1, end='\n', condition=optimization)
            else:
                for error in errors:
                    sys.stderr.write(error + '\n')
                sys.stderr.write('\n')
                sys.exit(1)

        else:
            start_time = datetime.datetime.now()
            report = {
                't': target_proteins_files,
                'q': query_patterns_files,
                'T': threads,
                'k': key_residue,
                'm': minkey,
                'M': maxkey,
                'l': minlength,
                'L': maxlength,
                'n': nterm,
                'c': cterm,
                'S': score_thr,
                'S1': score_thr_pass1,
                'S2': score_thr_pass2,
                'F': fusion_thr,
                'I': identity_str,
                'C': coverage_str,
                'b': by_pass,
                'sE': max_evalue,
                'sI': min_hit_pident,
                'sJ': min_shortest_seq_pident,
                'sP': min_hit_ppos,
                'sQ': min_shortest_seq_ppos,
                'sL': min_hit_length,
                'sR': reciprocity,
                'sC': min_coverage,
                'start_time': start_time,
                'o': file_id,
                's': subclustering,
                'kg': gain,
                'kl': loss,
                'i': allow_inclusion,
                'P': persistence_str,
                'K': config_file_name
            }

            filter_passed = True

            if not by_pass:

                (initial_target_proteins, seqIDs, samples,
                    fasta_headers, report) = import_target_proteins(
                    target_proteins_files, report, optimization)
                target_proteins, report = filter_target_proteins(
                    initial_target_proteins, fasta_headers, minlength,
                    maxlength, key_residue, minkey, maxkey, report,
                    optimization)

                if len(target_proteins) > 0:

                    if de_novo:
                        target_patterns = extract_target_patterns(
                            target_proteins, key_residue, nterm, cterm,
                            optimization)
                        (query_patterns, query_IDs,
                            report) = extract_de_novo_query_patterns(
                            target_patterns, nterm, cterm, seqIDs, report,
                            optimization)
                        mapping_scores = map_patterns(
                            query_patterns, target_patterns, identity_str,
                            coverage_str, persistence_str, score_thr, gain,
                            loss, allow_inclusion, de_novo, minkey,
                            export_scores, threads, optimization)
                        if export_scores:
                            export_mapping_scores(mapping_scores, seqIDs,
                                                  de_novo, optimization)
                        (clusters, sequence_edges,
                            node_attributes, report) = make_clusters(
                            mapping_scores, threads, score_thr_pass1,
                            score_thr_pass2, fusion_thr, report, optimization)
                        (clusters_table, clusters_stats, cytoscape_edges,
                            cytoscape_attributes) = prepare_clusters_info(
                            clusters, target_proteins, sequence_edges, seqIDs,
                            samples, node_attributes)
                        report = export_clusters_info(clusters_table,
                                                      clusters_stats, file_id,
                                                      report, optimization)
                        if graphs:
                            export_clusters_cytoscape(cytoscape_edges,
                                                      cytoscape_attributes,
                                                      file_id, optimization)
                        clusters_files, report = export_clusters_fasta(
                            clusters, target_proteins, fasta_headers, file_id,
                            report, optimization)

                    else:
                        target_patterns = extract_target_patterns(
                            target_proteins, key_residue, nterm, cterm,
                            optimization)
                        (query_patterns, query_IDs,
                            report) = extract_external_query_patterns(
                            query_patterns_files, nterm, cterm, key_residue,
                            report, optimization)
                        mapping_scores = map_patterns(
                            query_patterns, target_patterns, identity_str,
                            coverage_str, persistence_str, score_thr, gain,
                            loss, allow_inclusion, de_novo, minkey,
                            export_scores, threads, optimization)
                        if export_scores:
                            export_mapping_scores(mapping_scores, seqIDs,
                                                  de_novo, optimization)
                        groups = make_groups(mapping_scores, threads,
                                             score_thr, report, optimization)
                        groups_table, groups_stats = prepare_groups_info(
                            groups, target_proteins, seqIDs, samples)
                        report = export_groups_info(groups_table, groups_stats,
                                                    file_id, report,
                                                    optimization)
                        clusters_files = export_groups_fasta(
                            groups, target_proteins, fasta_headers, file_id,
                            optimization)

            else:
                clusters_files = target_proteins_files

            if subclustering:
                report['s'] = True
                (subclusters_tables, subclusters_stats,
                    cytoscape_edges, cytoscape_attributes,
                    all_samples, report) = make_subclusters(
                    clusters_files, threads, file_id, max_evalue,
                    min_hit_pident, min_shortest_seq_pident, min_hit_ppos,
                    min_shortest_seq_ppos, min_hit_length, reciprocity,
                    min_coverage, makeblastdb_path, blastp_path,
                    blastp_options, report, optimization)
                report = export_subclusters_info(
                    subclusters_tables, subclusters_stats, all_samples,
                    file_id, report, optimization)
                if graphs:
                    export_subclusters_cytoscape(cytoscape_edges,
                                                 cytoscape_attributes, file_id,
                                                 optimization)

            stop_time = datetime.datetime.now()
            execution_time = stop_time - start_time

            report['stop_time'] = stop_time
            report['execution_time'] = execution_time

            print_stdout(
                'Execution finished. Time elapsed: ' + str(execution_time),
                level=0,
                end='\n',
                condition=not optimization)

            if optimization:
                optimization_reports[n] = report
                remove_outputs(file_id)
            elif make_report:
                write_execution_report(report)

            print_stdout('Done.', level=1, end='\n', condition=optimization)

    if optimization:
        write_optimization_report(optimization_reports, file_id)
